# llm_module.py
import re
import logging
import os
import sys
import time
import json
import uuid
from typing import Generator, List, Dict, Optional, Any
from threading import Lock

# LangChain ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
from langchain_core.exceptions import OutputParserException

# --- ê¸°ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ì¡´ì„± (í•„ìš” ì‹œ ìœ ì§€) ---
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
log_level_str = os.getenv("LOG_LEVEL", "INFO").upper()
log_level = getattr(logging, log_level_str, logging.INFO)
if not logging.getLogger().handlers:
    logging.basicConfig(level=log_level,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                        stream=sys.stdout)
logger = logging.getLogger(__name__)
logger.setLevel(log_level)

# --- í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ---
try:
    from dotenv import load_dotenv
    load_dotenv()
    logger.debug("ğŸ¤–âš™ï¸ .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
except ImportError:
    logger.debug("ğŸ¤–âš™ï¸ python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ .env ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

# LangSmith ì¶”ì ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = os.getenv("LANGCHAIN_PROJECT", "default-project")
# LANGCHAIN_API_KEYëŠ” .env íŒŒì¼ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://127.0.0.1:11434")
LMSTUDIO_BASE_URL = os.getenv("LMSTUDIO_BASE_URL", "http://127.0.0.1:1234/v1")


class LLM:
    """
    LangChainì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ LLM ë°±ì—”ë“œì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•œ í†µí•© ì¸í„°í˜ì´ìŠ¤.
    Ollama, OpenAI, LMStudioë¥¼ ì§€ì›í•˜ë©° LangSmith ì¶”ì ì´ ë‚´ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    """
    SUPPORTED_BACKENDS = ["ollama", "openai", "lmstudio"]

    def __init__(
        self,
        backend: str,
        model: str,
        system_prompt: Optional[str] = None,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        no_think: bool = False,
    ):
        logger.info(f"ğŸ¤–âš™ï¸ LLM ì´ˆê¸°í™” ì¤‘: ë°±ì—”ë“œ={backend}, ëª¨ë¸={model}")
        self.backend = backend.lower()
        if self.backend not in self.SUPPORTED_BACKENDS:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°±ì—” '{backend}'. ì§€ì›ë˜ëŠ” ë°±ì—”ë“œ: {self.SUPPORTED_BACKENDS}")

        self.model_name = model
        self.system_prompt = system_prompt
        self.no_think = no_think
        self.client = self._create_langchain_client(backend, model, api_key, base_url)
        
        self.system_prompt_message = None
        if self.system_prompt:
            self.system_prompt_message = SystemMessage(content=self.system_prompt)
            logger.info("ğŸ¤–ğŸ’¬ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def _create_langchain_client(self, backend, model, api_key, base_url):
        if backend == "openai":
            key = api_key or OPENAI_API_KEY
            if not key:
                raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•˜ì§€ë§Œ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return ChatOpenAI(api_key=key, model=model, temperature=0.7, streaming=True)
        
        if backend == "ollama":
            url = base_url or OLLAMA_BASE_URL
            return ChatOllama(base_url=url, model=model, temperature=0.7)

        if backend == "lmstudio":
            url = base_url or LMSTUDIO_BASE_URL
            return ChatOpenAI(api_key="no-key-needed", base_url=url, model=model, temperature=0.7, streaming=True)

        raise ValueError(f"ë°±ì—”ë“œ '{backend}'ì— ëŒ€í•œ LangChain í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def generate(
        self,
        text: str,
        history: Optional[List[Dict[str, str]]] = None,
        use_system_prompt: bool = True,
        request_id: Optional[str] = None, # LangChainì—ì„œ ìë™ ì²˜ë¦¬ë˜ë¯€ë¡œ ìœ ì§€í•  í•„ìš” ì—†ìŒ
        **kwargs: Any
    ) -> Generator[str, None, None]:
        
        logger.info(f"ğŸ¤–ğŸ’¬ ìƒì„± ì‹œì‘ (ë°±ì—”ë“œ: {self.backend})")
        messages = []
        if use_system_prompt and self.system_prompt_message:
            messages.append(self.system_prompt_message)
        
        if history:
            for h in history:
                if h["role"] == "user":
                    messages.append(HumanMessage(content=h["content"]))
                elif h["role"] == "assistant":
                    # LangChainì€ AIMessageë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, í˜¸í™˜ì„±ì„ ìœ„í•´ Humanìœ¼ë¡œ ì²˜ë¦¬
                    messages.append(HumanMessage(content=h["content"]))

        added_text = text
        if self.no_think:
            added_text = f"{text}/nothink"
        messages.append(HumanMessage(content=added_text))

        try:
            for chunk in self.client.stream(messages, **kwargs):
                yield chunk.content
        except Exception as e:
            logger.error(f"ğŸ¤–ğŸ’¥ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            raise

    def prewarm(self, max_retries: int = 1) -> bool:
        """LLM ì—°ê²°ì„ ì˜ˆì—´í•˜ê³  ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤."""
        prompt = "Respond with only the word 'OK'."
        logger.info(f"ğŸ¤–ğŸ”¥ ë°±ì—”ë“œ '{self.backend}'ì˜ ëª¨ë¸ '{self.model_name}' ì˜ˆì—´ ì‹œë„ ì¤‘...")
        
        for attempt in range(max_retries + 1):
            try:
                response = self.client.invoke([HumanMessage(content=prompt)])
                if response.content:
                    logger.info(f"ğŸ¤–ğŸ”¥âœ… ì˜ˆì—´ ì„±ê³µ. ì‘ë‹µ: '{response.content}'")
                    return True
            except Exception as e:
                logger.warning(f"ğŸ¤–ğŸ”¥âš ï¸ ì˜ˆì—´ ì‹œë„ {attempt + 1}/{max_retries + 1} ì‹¤íŒ¨: {e}")
                if attempt < max_retries:
                    time.sleep(2 * (attempt + 1))
                else:
                    logger.error("ğŸ¤–ğŸ”¥ğŸ’¥ ëª¨ë“  ì¬ì‹œë„ í›„ ì˜ˆì—´ ì‹¤íŒ¨.")
                    return False
        return False

    def measure_inference_time(
        self,
        num_tokens: int = 10,
        **kwargs: Any
    ) -> Optional[float]:
        """ëª©í‘œ í† í° ìˆ˜ ìƒì„±ì„ ìœ„í•œ ì¶”ë¡  ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤."""
        # LangChainì˜ ì¶”ìƒí™”ë¡œ ì¸í•´ ì •í™•í•œ í† í° ê¸°ë°˜ ì‹œê°„ ì¸¡ì •ì´ ë³µì¡í•´ì¡ŒìŠµë‹ˆë‹¤.
        # ëŒ€ì‹  ê°„ë‹¨í•œ invoke í˜¸ì¶œ ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
        logger.info(f"ğŸ¤–â±ï¸ ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¤‘ (ê°„ë‹¨í•œ invoke í˜¸ì¶œ)...")
        
        measurement_user_prompt = "Repeat the following sequence exactly, word for word: one two three four five six seven eight nine ten eleven twelve"
        messages = [HumanMessage(content=measurement_user_prompt)]
        
        start_time = time.time()
        try:
            self.client.invoke(messages, **kwargs)
            end_time = time.time()
            duration_ms = (end_time - start_time) * 1000
            logger.info(f"ğŸ¤–â±ï¸âœ… ì¸¡ì •ëœ ì¶”ë¡  ì‹œê°„: {duration_ms:.2f} ms")
            return duration_ms
        except Exception as e:
            logger.error(f"ğŸ¤–â±ï¸ğŸ’¥ ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¤‘ ì˜¤ë¥˜: {e}", exc_info=False)
            return None

# --- ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ìëŠ” ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŒ (LangChainì´ ì²˜ë¦¬) ---

# --- ì‚¬ìš© ì˜ˆì‹œ ---
if __name__ == "__main__":
    main_logger = logging.getLogger(__name__)
    main_logger.info("ğŸ¤–ğŸš€ --- LLM ëª¨ë“ˆ (LangChain) ì˜ˆì‹œ ì‹¤í–‰ ---")

    # --- Ollama ì˜ˆì‹œ ---
    try:
        ollama_model_env = os.getenv("OLLAMA_MODEL", "llama3:instruct")
        main_logger.info(f"\nğŸ¤–âš™ï¸ --- Ollama ({ollama_model_env}) ì´ˆê¸°í™” ì¤‘ ---")
        ollama_llm = LLM(
            backend="ollama",
            model=ollama_model_env,
            system_prompt="You are a concise and helpful assistant."
        )

        main_logger.info("ğŸ¤–ğŸ”¥ --- Ollama ì˜ˆì—´ ì‹¤í–‰ ì¤‘ ---")
        if ollama_llm.prewarm():
            main_logger.info("ğŸ¤–âœ… Ollama ì˜ˆì—´/ì´ˆê¸°í™” ì •ìƒ.")
            
            main_logger.info("ğŸ¤–â±ï¸ --- Ollama ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì‹¤í–‰ ì¤‘ ---")
            inf_time = ollama_llm.measure_inference_time()
            if inf_time is not None:
                main_logger.info(f"ğŸ¤–â±ï¸ --- ì¸¡ì •ëœ ì¶”ë¡  ì‹œê°„: {inf_time:.2f} ms ---")

            main_logger.info("ğŸ¤–â–¶ï¸ --- Ollama ìƒì„± ì‹¤í–‰ ì¤‘ ---")
            try:
                generator = ollama_llm.generate("What is the capital of France? Be brief.")
                print("\nOllama Response: ", end="", flush=True)
                for token in generator:
                    print(token, end="", flush=True)
                print("\n")
                main_logger.info("ğŸ¤–âœ… Ollama ìƒì„± ì™„ë£Œ.")
            except Exception as e:
                main_logger.error(f"ğŸ¤–ğŸ’¥ Ollama ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)
        else:
            main_logger.error("ğŸ¤–âŒ Ollama ì˜ˆì—´/ì´ˆê¸°í™” ì‹¤íŒ¨.")

    except Exception as e:
        main_logger.error(f"ğŸ¤–ğŸ’¥ Ollama ì˜ˆì‹œ ì‹¤íŒ¨: {e}", exc_info=True)

    main_logger.info("\n" + "="*40)
    main_logger.info("ğŸ¤–ğŸ --- LLM ëª¨ë“ˆ ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ ---")