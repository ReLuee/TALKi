# speech_pipeline_manager.py
from typing import Optional, Callable, List, Dict
import threading
import logging
import time
from queue import Queue, Empty
import sys
import re

# AI-to-AI ëŒ€í™” ì œì–´ ì„¤ì •
DISCONN_ALARM = True          # ì—°ê²° ëŠê¹€ ì•Œë¦¼ í™œì„±í™”
MAXIMUM_AI_TO_AI = 15         # ìµœëŒ€ AI-to-AI ëŒ€í™” íšŸìˆ˜ (0 = ë¬´ì œí•œ)
AI_TURN_DELAY = 0.1           # AI í„´ ì‚¬ì´ ì§€ì—° ì‹œê°„ (ì´ˆ)

# LangChain ë° RAG ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document

from audio_module import AudioProcessor
from text_similarity import TextSimilarity
from text_context import TextContext
from llm_module import LLM
from colors import Colors

logger = logging.getLogger(__name__)

def load_prompt_from_file(file_path: str, default_prompt: str) -> str:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            prompt = f.read().strip()
        logger.info(f"ğŸ—£ï¸ğŸ“„ íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {file_path}")
        return prompt
    except FileNotFoundError:
        logger.warning(f"ğŸ—£ï¸ğŸ“„ {file_path}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return default_prompt

# í˜ë¥´ì†Œë‚˜ ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í”„ë¡¬í”„íŠ¸ ë¡œë“œ
persona_ai1 = load_prompt_from_file("persona_ai1.txt", "You are a friendly conversation partner.")
persona_ai2 = load_prompt_from_file("persona_ai2.txt", "You are a helpful tutor who corrects mistakes.")
orchestrator_prompt_text = load_prompt_from_file("orchestrator.txt", "You are an orchestrator. Decide who should speak next: AI1_friend or AI2_tutor.")

# ... (ê¸°ì¡´ orpheus_prompt_addon ì½”ë“œ ìœ ì§€)
USE_ORPHEUS_UNCENSORED = False
orpheus_prompt_addon_normal = """
When expressing emotions, you are ONLY allowed to use the following exact tags (including the spaces):
" <laugh> ", " <chuckle> ", " <sigh> ", " <cough> ", " <sniffle> ", " <groan> ", " <yawn> ", and " <gasp> ".
Do NOT create or use any other emotion tags. Do NOT remove the spaces. Use these tags exactly as shown, and only when appropriate.
""".strip()
orpheus_prompt_addon_uncensored = """
When expressing emotions, you are ONLY allowed to use the following exact tags (including the spaces):
" <moans> ", " <panting> ", " <grunting> ", " <gagging sounds> ", " <chokeing> ", " <kissing noises> ", " <laugh> ", " <chuckle> ", " <sigh> ", " <cough> ", " <sniffle> ", " <groan> ", " <yawn> ", " <gasp> ".
Do NOT create or use any other emotion tags. Do NOT remove the spaces. Use these tags exactly as shown, and only when appropriate.
""".strip()
orpheus_prompt_addon = orpheus_prompt_addon_uncensored if USE_ORPHEUS_UNCENSORED else orpheus_prompt_addon_normal


class PipelineRequest:
    def __init__(self, action: str, data: Optional[any] = None):
        self.action = action
        self.data = data
        self.timestamp = time.time()

class RunningGeneration:
    def __init__(self, id: int):
        self.id: int = id
        self.text: Optional[str] = None
        self.speaker_role: str = "assistant"
        self.timestamp = time.time()
        self.llm_generator = None
        self.llm_finished: bool = False
        self.llm_finished_event = threading.Event()
        self.llm_aborted: bool = False
        self.quick_answer: str = ""
        self.quick_answer_provided: bool = False
        self.quick_answer_first_chunk_ready: bool = False
        self.quick_answer_overhang: str = ""
        self.tts_quick_started: bool = False
        self.tts_quick_allowed_event = threading.Event()
        self.audio_chunks = Queue()
        self.total_audio_duration: float = 0.0
        self.audio_quick_finished: bool = False
        self.audio_quick_aborted: bool = False
        self.tts_quick_finished_event = threading.Event()
        self.abortion_started: bool = False
        self.tts_final_finished_event = threading.Event()
        self.tts_final_started: bool = False
        self.audio_final_aborted: bool = False
        self.audio_final_finished: bool = False
        self.final_answer: str = ""
        self.completed: bool = False

class SpeechPipelineManager:
    def __init__(
            self,
            tts_engine: str = "coqui",
            llm_provider: str = "openai",
            llm_model: str = "gpt-4o",
            no_think: bool = False,
            orpheus_model: str = "orpheus-3b-0.1-ft-Q8_0-GGUF/orpheus-3b-0.1-ft-q8_0.gguf",
        ):
        self.tts_engine = tts_engine
        self.llm_provider = llm_provider
        self.llm_model = llm_model
        self.no_think = no_think
        self.orpheus_model = orpheus_model

        self.audio = AudioProcessor(engine=self.tts_engine, orpheus_model=self.orpheus_model)
        self.audio.on_first_audio_chunk_synthesize = self.on_first_audio_chunk_synthesize
        self.text_similarity = TextSimilarity(focus='end', n_words=5)
        self.text_context = TextContext()
        self.generation_counter: int = 0
        self.abort_lock = threading.Lock()
        
        self.llm = None
        self.rag_chain_ai1 = None
        self.rag_chain_ai2 = None
        self.orchestrator_chain = None
        self._setup_chains()

        self.llm.prewarm()
        self.llm_inference_time = self.llm.measure_inference_time()
        logger.debug(f"ğŸ—£ï¸ğŸ§ ğŸ•’ LLM ì¶”ë¡  ì‹œê°„: {self.llm_inference_time:.2f}ms")

        self.history = []
        self.requests_queue = Queue()
        self.running_generation: Optional[RunningGeneration] = None
        self.shutdown_event = threading.Event()
        self.generator_ready_event = threading.Event()
        self.llm_answer_ready_event = threading.Event()
        self.stop_everything_event = threading.Event()
        self.stop_llm_request_event = threading.Event()
        self.stop_llm_finished_event = threading.Event()
        self.stop_tts_quick_request_event = threading.Event()
        self.stop_tts_quick_finished_event = threading.Event()
        self.stop_tts_final_request_event = threading.Event()
        self.stop_tts_final_finished_event = threading.Event()
        self.abort_completed_event = threading.Event()
        self.abort_block_event = threading.Event()
        self.abort_block_event.set()
        self.check_abort_lock = threading.Lock()
        self.llm_generation_active = False
        self.tts_quick_generation_active = False
        self.tts_final_generation_active = False
        self.previous_request = None

        self.request_processing_thread = threading.Thread(target=self._request_processing_worker, name="RequestProcessingThread", daemon=True)
        self.llm_inference_thread = threading.Thread(target=self._llm_inference_worker, name="LLMProcessingThread", daemon=True)
        self.tts_quick_inference_thread = threading.Thread(target=self._tts_quick_inference_worker, name="TTSQuickProcessingThread", daemon=True)
        self.tts_final_inference_thread = threading.Thread(target=self._tts_final_inference_worker, name="TTSFinalProcessingThread", daemon=True)
        self.ai_to_ai_worker_thread = threading.Thread(target=self._ai_to_ai_worker, name="AIToAIWorkerThread", daemon=True)
        
        self.ai_to_ai_active = False
        self.ai_to_ai_paused = False
        self.last_ai_speaker = "AI2_tutor" # AI1ì´ ë¨¼ì € ì‹œì‘í•˜ë„ë¡ ì´ˆê¸°í™”
        self.last_ai_response = ""
        self.ai_to_ai_turn_count = 0  # AI-to-AI ëŒ€í™” íšŸìˆ˜ ì¹´ìš´í„°
        self.client_connected = True  # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ
        self.ai_conversation_interrupted = False  # AI ëŒ€í™” ì¤‘ë‹¨ ìƒíƒœ
        self.ai_to_ai_session_limit_reached = False  # ì„¸ì…˜ë³„ ëŒ€í™” ì œí•œ ë„ë‹¬ ìƒíƒœ
        
        # í´ë¼ì´ì–¸íŠ¸ TTS ì¬ìƒ ì™„ë£Œ ëŒ€ê¸°ë¥¼ ìœ„í•œ ì½œë°± ì°¸ì¡°
        self.tts_playback_finished_callback = None

        self.request_processing_thread.start()
        self.llm_inference_thread.start()
        self.tts_quick_inference_thread.start()
        self.tts_final_inference_thread.start()
        self.ai_to_ai_worker_thread.start()

        self.on_partial_assistant_text: Optional[Callable[[str], None]] = None
        self.on_final_assistant_answer: Optional[Callable[..., None]] = None
        self.on_ai_turn_start: Optional[Callable[[], None]] = None
        self.on_tts_all_complete: Optional[Callable[[], None]] = None
        self.full_output_pipeline_latency = (self.llm_inference_time or 0) + self.audio.tts_inference_time
        logger.info(f"ğŸ—£ï¸â±ï¸ ì „ì²´ ì¶œë ¥ íŒŒì´í”„ë¼ì¸ ì§€ì—° ì‹œê°„: {self.full_output_pipeline_latency:.2f}ms (LLM: {self.llm_inference_time or 0:.2f}ms, TTS: {self.audio.tts_inference_time:.2f}ms)")
        logger.info("ğŸ—£ï¸ğŸš€ SpeechPipelineManagerê°€ ì´ˆê¸°í™”ë˜ì—ˆê³  ì›Œì»¤ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def toggle_pause_ai_conversation(self):
        """AI ê°„ ëŒ€í™”ì˜ ì¼ì‹œì¤‘ì§€ ìƒíƒœë¥¼ í† ê¸€í•©ë‹ˆë‹¤."""
        self.ai_to_ai_paused = not self.ai_to_ai_paused
        state = "ì¼ì‹œì¤‘ì§€ë¨" if self.ai_to_ai_paused else "ì¬ê°œë¨"
        logger.info(f"ğŸ¤–â¯ï¸ AI ê°„ ëŒ€í™”ê°€ {state} ìƒíƒœë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return self.ai_to_ai_paused

    def set_client_connection_status(self, connected: bool):
        """í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        self.client_connected = connected
        if not connected:
            logger.info("ğŸ”ŒâŒ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤. AI-to-AI ëŒ€í™” ì¤‘ë‹¨ì„ ìœ„í•´ ëŒ€ê¸° ì¤‘...")
            if self.ai_to_ai_active:
                self.ai_to_ai_active = False
                self.ai_conversation_interrupted = True
                logger.info("ğŸ¤–ğŸ›‘ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€ìœ¼ë¡œ AI-to-AI ëŒ€í™”ë¥¼ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        else:
            logger.info("ğŸ”Œâœ… í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
            # ì—°ê²° ëŠê¹€ ì•Œë¦¼ ê¸°ëŠ¥
            if DISCONN_ALARM and self.ai_conversation_interrupted:
                self.send_disconnection_notification()
                self.ai_conversation_interrupted = False

    def send_disconnection_notification(self):
        """ì—°ê²° ëŠê¹€ìœ¼ë¡œ ì¸í•œ AI ëŒ€í™” ì¤‘ë‹¨ ì•Œë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤."""
        if self.on_final_assistant_answer:
            logger.info("ğŸ”” ì—°ê²° ëŠê¹€ ì•Œë¦¼ ì „ì†¡ ì¤‘...")
            notification_message = "AI ëŒ€í™”ê°€ ì—°ê²° ëŠê¹€ìœ¼ë¡œ ì¸í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
            
            # íˆìŠ¤í† ë¦¬ì— ì•Œë¦¼ ë©”ì‹œì§€ ì¶”ê°€
            self.history.append({
                "role": "system",
                "content": notification_message
            })
            
            # í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì•Œë¦¼ ì „ì†¡
            self.on_final_assistant_answer(
                forced=True,
                custom_message=notification_message,
                speaker_role_override="system"
            )

    def handle_prolonged_silence(self):
        """ì‚¬ìš©ìì˜ ê¸´ ì¹¨ë¬µì„ ì²˜ë¦¬í•˜ê³  AI ê°„ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        if self.ai_to_ai_active or self.is_valid_gen():
            logger.info("ğŸ¤–ğŸ’¬ ê¸´ ì¹¨ë¬µ ê°ì§€ë¨, í•˜ì§€ë§Œ AI ëŒ€í™”ê°€ ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆê±°ë‚˜ ìƒì„±ì´ ì§„í–‰ ì¤‘ì´ë¯€ë¡œ ë¬´ì‹œí•©ë‹ˆë‹¤.")
            return
        elif self.ai_to_ai_paused:
            logger.info("AI-to-AI ê¸°ëŠ¥ ì¼ì‹œì •ì§€ ì¤‘ì´ë¯€ë¡œ ì¥ì‹œê°„ ì¹¨ë¬µ íƒ€ì´ë¨¸ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.")
            return
        elif self.ai_to_ai_session_limit_reached:
            logger.info("ğŸ¤–ğŸ“Š ì´ ì„¸ì…˜ì—ì„œ AI-to-AI ëŒ€í™” ì œí•œì— ì´ë¯¸ ë„ë‹¬í–ˆìœ¼ë¯€ë¡œ ì¹¨ë¬µ íƒ€ì´ë¨¸ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.")
            return
        
        logger.info("ğŸ¤–ğŸ’¬ ê¸´ ì¹¨ë¬µìœ¼ë¡œ ì¸í•´ AI ê°„ ëŒ€í™” ì‹œì‘ ì¤‘...")
        
        # ëŒ€í™” ê¸°ë¡ì—ì„œ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì™€ ì£¼ì œë¡œ ì‚¬ìš©
        if self.history:
            last_message = self.history[-1]
            # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ ë‚´ìš©ì„ ì£¼ì œ/ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
            topic = last_message.get("content", "What do you think about that?")
        else:
            # ê¸°ë¡ì´ ë¹„ì–´ìˆì„ ê²½ìš°ì˜ ëŒ€ì²´ ì£¼ì œ
            topic = "Let's talk about something interesting."
            
        self.start_ai_to_ai_conversation(topic)

    def start_ai_to_ai_conversation(self, topic: str, manual_start: bool = False):
        logger.info(f"ğŸ¤–ğŸ’¬ AI ê°„ ëŒ€í™” ì‹œì‘ (ì£¼ì œ: {topic})")
        self.ai_to_ai_active = True
        self.ai_to_ai_paused = False # ëŒ€í™” ì‹œì‘ ì‹œ í•­ìƒ ì¼ì‹œì¤‘ì§€ í•´ì œ
        self.ai_to_ai_turn_count = 0  # ëŒ€í™” ì‹œì‘ ì‹œ ì¹´ìš´í„° ë¦¬ì…‹
        
        # ìˆ˜ë™ ì‹œì‘ì˜ ê²½ìš° ì„¸ì…˜ ì œí•œ ë¦¬ì…‹
        if manual_start:
            self.ai_to_ai_session_limit_reached = False
            logger.info("ğŸ¤–ğŸ”„ ìˆ˜ë™ ì‹œì‘ìœ¼ë¡œ ì¸í•´ ì„¸ì…˜ ì œí•œ ë¦¬ì…‹ë¨")
            
        self.prepare_generation(topic, is_ai_turn=True)
        
    def wait_for_client_tts_finish(self, timeout: float = 10.0) -> bool:
        """í´ë¼ì´ì–¸íŠ¸ TTS ì¬ìƒ ì™„ë£Œë¥¼ ëŒ€ê¸°í•©ë‹ˆë‹¤."""
        if self.tts_playback_finished_callback:
            return self.tts_playback_finished_callback.tts_playback_finished_event.wait(timeout)
        return False

    def _ai_to_ai_worker(self):
        logger.info("ğŸ¤–ğŸ’¬ AI-to-AI ì›Œì»¤ ì‹œì‘ ì¤‘...")
        while not self.shutdown_event.is_set():
            if not self.ai_to_ai_active:
                time.sleep(0.1)
                continue

            # --- ì—°ê²° ìƒíƒœ ì²´í¬ ---
            if not self.client_connected:
                logger.info("ğŸ¤–ğŸ”Œ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€ìœ¼ë¡œ AI-to-AI ëŒ€í™” ì¤‘ë‹¨")
                self.ai_to_ai_active = False
                continue
            
            
            # --- ì¼ì‹œì¤‘ì§€ ë¡œì§ ---
            while self.ai_to_ai_paused:
                if self.shutdown_event.is_set() or not self.ai_to_ai_active:
                    break
                time.sleep(0.1)
            # --------------------

            # TTS ì¬ìƒ ì™„ë£Œ ëŒ€ê¸°ë¥¼ ìœ„í•œ ì½œë°± í™•ì¸
            if self.tts_playback_finished_callback and self.tts_playback_finished_callback.tts_playback_finished_event.wait(timeout=0.1):
                if not self.ai_to_ai_active:
                    continue
                
                logger.info("ğŸ¤–ğŸ’¬ í´ë¼ì´ì–¸íŠ¸ TTS ì¬ìƒ ì™„ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ë¨")
                
                # ì´ë²¤íŠ¸ í´ë¦¬ì–´ ë° ë‹¤ìŒ í„´ ì²˜ë¦¬
                self.tts_playback_finished_callback.tts_playback_finished_event.clear()
 
                # AI-to-AI ìƒíƒœ ì¬í™•ì¸ (ì‚¬ìš©ì interrupt ë°©ì§€)
                if self.ai_to_ai_active:
                    # ë‹¤ìŒ ë°˜ë³µ ì „ì— ì¼ì‹œì¤‘ì§€ ìƒíƒœ ë‹¤ì‹œ í™•ì¸
                    if self.ai_to_ai_paused:
                        continue
                    
                    # AI í„´ ì¹´ìš´í„° ì¦ê°€ (ì´ì „ AI í„´ì´ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ)
                    self.ai_to_ai_turn_count += 1
                    logger.info(f"ğŸ¤–ğŸ“Š AI-to-AI í„´ ì¹´ìš´í„°: {self.ai_to_ai_turn_count}")
                    
                    # í„´ ì¹´ìš´í„° ì¦ê°€ í›„ ì¦‰ì‹œ ì œí•œ í™•ì¸
                    if MAXIMUM_AI_TO_AI > 0 and self.ai_to_ai_turn_count >= MAXIMUM_AI_TO_AI:
                        logger.info(f"ğŸ¤–ğŸ“Š AI-to-AI ëŒ€í™” íšŸìˆ˜ ì œí•œ ë„ë‹¬ ({self.ai_to_ai_turn_count}/{MAXIMUM_AI_TO_AI})")
                        self.ai_to_ai_active = False
                        self.ai_to_ai_session_limit_reached = True
                        
                        # ì œí•œ ë„ë‹¬ ì•Œë¦¼ ì „ì†¡
                        if self.on_final_assistant_answer:
                            limit_message = f"AI ëŒ€í™”ê°€ ìµœëŒ€ íšŸìˆ˜ ì œí•œ({MAXIMUM_AI_TO_AI}íšŒ)ì— ë„ë‹¬í•˜ì—¬ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì„¸ì…˜ì—ì„œëŠ” ë” ì´ìƒ AI ëŒ€í™”ê°€ ìë™ìœ¼ë¡œ ì‹œì‘ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                            self.history.append({
                                "role": "system",
                                "content": limit_message
                            })
                            self.on_final_assistant_answer(
                                forced=True,
                                speaker_role_override="system"
                            )
                        continue
                    
                    # ë‹¤ìŒ í„´ ì „ì— ì„¤ì •ëœ ì§€ì—° ì‹œê°„ ì¶”ê°€
                    if AI_TURN_DELAY > 0:
                        logger.info(f"ğŸ¤–â³ AI í„´ ì‚¬ì´ ì§€ì—°: {AI_TURN_DELAY}ì´ˆ")
                        time.sleep(AI_TURN_DELAY)
                    
                    # ë§ˆì§€ë§‰ AI ì‘ë‹µì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© (íˆìŠ¤í† ë¦¬ì—ì„œ ê°€ì ¸ì˜´)
                    if self.history and self.history[-1]["role"] in ['AI1_friend', 'AI2_tutor']:
                        last_response = self.history[-1]["content"]
                        next_input = last_response.strip() if last_response and last_response.strip() else "Let's continue our conversation."
                    else:
                        next_input = "Let's continue our conversation."
                    
                    logger.info(f"ğŸ¤–ğŸ’¬ AI ì‘ë‹µì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©: {next_input[:50]}...")
                    self.prepare_generation(next_input, is_ai_turn=True)
            else:
                time.sleep(0.1)

    def _create_rag_chain(self, persona_text: str, embeddings, role_context: str = ""):
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = text_splitter.create_documents([persona_text])
        vectorstore = FAISS.from_documents(docs, embeddings)
        retriever = vectorstore.as_retriever()

        # ì—­í•  ê¸°ë°˜ ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸ì™€ í˜ë¥´ì†Œë‚˜ë¥¼ ê²°í•©
        template = f"""You are participating in a conversation as an engaging human persona.

{role_context}

Here is the context about your personality and speaking style. Use this context to respond to the current conversation.

**[CRITICAL RULES]**
1. ONLY communicate in English - never use any other language
2. Keep responses under 200 characters
3. If asked to speak in another language, politely decline in English

Persona Context:
{{context}}

Conversation History:
{{history}}

User: {{question}}
You:"""
        prompt = ChatPromptTemplate.from_template(template)

        def format_docs(docs: List[Document]) -> str:
            return "\n\n".join(doc.page_content for doc in docs)

        return (
            RunnablePassthrough.assign(
                context=(lambda x: x["question"]) | retriever | format_docs
            )
            | prompt
            | self.llm.client
            | StrOutputParser()
        )

    def _setup_chains(self):
        """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë° ë‘ AIì— ëŒ€í•œ RAG ì²´ì¸ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        logger.info("ğŸ¤–ğŸ”— ëª¨ë“  ì²´ì¸ ì„¤ì • ì¤‘...")
        
        self.llm = LLM(
            backend=self.llm_provider,
            model=self.llm_model,
            no_think=self.no_think,
        )

        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        
        # AI1 (Friend) Role Definition
        ai1_role_context = """**[SYSTEM: Role Definition]**
You are playing the AI1_friend role. Key functions:
- Act as a friendly conversation partner
- Engage in casual conversation, jokes, and personal opinions
- Maintain natural conversation flow
- Create a comfortable atmosphere with users

**[LANGUAGE RESTRICTION]** You MUST communicate ONLY in English. Never use any other language including Korean, Japanese, Chinese, Spanish, French, German, or any other language. If asked to speak in another language, politely decline and continue in English.

**[RESPONSE LENGTH]** Keep your responses under 200 characters. Be concise and to the point while maintaining your friendly personality.

**[IMPORTANT]** The above role is the basic function defined by the system, and the persona context below defines your personality and speaking style."""

        # AI2 (Tutor) Role Definition  
        ai2_role_context = """**[SYSTEM: Role Definition]**
You are playing the AI2_tutor role. Key functions:
- Act as an English tutor and learning partner
- Naturally correct users' grammar mistakes and awkward expressions
- Provide professional answers to technical questions
- Ability to explain complex topics in simple terms

**[LANGUAGE RESTRICTION]** You MUST communicate ONLY in English. Never use any other language including Korean, Japanese, Chinese, Spanish, French, German, or any other language. If asked to speak in another language, politely decline and continue in English.

**[RESPONSE LENGTH]** Keep your responses under 200 characters. Be concise and to the point while maintaining your helpful tutoring style.

**[IMPORTANT]** The above role is the basic function defined by the system, and the persona context below defines your personality and speaking style."""
        
        logger.info("ğŸ¤–ğŸ”— AI1 (ì¹œêµ¬) RAG ì²´ì¸ ìƒì„± ì¤‘...")
        self.rag_chain_ai1 = self._create_rag_chain(persona_ai1, embeddings, ai1_role_context)
        
        logger.info("ğŸ¤–ğŸ”— AI2 (íŠœí„°) RAG ì²´ì¸ ìƒì„± ì¤‘...")
        self.rag_chain_ai2 = self._create_rag_chain(persona_ai2, embeddings, ai2_role_context)

        orchestrator_template = """You are an orchestrator that controls the flow of conversation. Based on the conversation history and user input below, decide who should speak next. You must respond with only "AI1_friend" or "AI2_tutor".\n\n- "AI1_friend" is a friendly conversation partner.\n- "AI2_tutor" is a smart friend who corrects grammar and expressions.\n\nConversation History:\n{history}\n\nUser: {question}\nNext speaker:"""
        orchestrator_prompt = ChatPromptTemplate.from_template(orchestrator_template)
        self.orchestrator_chain = orchestrator_prompt | self.llm.client | StrOutputParser()
        
        logger.info("ğŸ¤–ğŸ”—âœ… ëª¨ë“  ì²´ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def process_prepare_generation(self, data: dict):
        txt = data.get("text", "")
        is_ai_turn = data.get("is_ai_turn", False)

        # AI í„´ì´ ì•„ë‹ ë•Œë§Œ ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ê°„ì£¼í•˜ê³  AI ëŒ€í™” ì¤‘ë‹¨
        if not is_ai_turn:
            self.ai_to_ai_active = False
            # ì‚¬ìš©ì ì…ë ¥ ì‹œ AI ëŒ€í™” ì¹´ìš´í„° ë° ì„¸ì…˜ ì œí•œ ë¦¬ì…‹
            self.ai_to_ai_turn_count = 0
            self.ai_to_ai_session_limit_reached = False
            logger.info("ğŸ¤–ğŸ”„ ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ì¸í•´ AI ëŒ€í™” ì¹´ìš´í„° ë° ì„¸ì…˜ ì œí•œ ë¦¬ì…‹ë¨")
        
        id_in_spec = self.generation_counter + 1
        
        # AI í„´ì´ë“  ì‚¬ìš©ì í„´ì´ë“ , ìƒˆë¡œìš´ ìƒì„±ì„ ì‹œì‘í•˜ê¸° ì „ì— í•­ìƒ ì´ì „ ìƒì„±ì„ ì¤‘ë‹¨/ì •ë¦¬í•©ë‹ˆë‹¤.
        self.check_abort(txt, wait_for_finish=True, abort_reason=f"process_prepare_generation for new id {id_in_spec}")

        self.generation_counter += 1
        new_gen_id = self.generation_counter
        logger.info(f"ğŸ—£ï¸âœ¨ğŸ”„ [Gen {new_gen_id}] ìƒˆ ìƒì„± ì¤€ë¹„ ì¤‘: '{txt[:50]}...' ")

        # ... (ìƒíƒœ ë¦¬ì…‹ ë¡œì§ì€ ë™ì¼)
        self.llm_generation_active = False
        self.tts_quick_generation_active = False
        self.tts_final_generation_active = False
        self.llm_answer_ready_event.clear()
        self.generator_ready_event.clear()
        self.stop_llm_request_event.clear()
        self.stop_llm_finished_event.clear()
        self.stop_tts_quick_request_event.clear()
        self.stop_tts_quick_finished_event.clear()
        self.stop_tts_final_request_event.clear()
        self.stop_tts_final_finished_event.clear()
        self.abort_completed_event.clear()
        self.abort_block_event.set()

        self.running_generation = RunningGeneration(id=new_gen_id)
        self.running_generation.text = txt

        if self.ai_to_ai_active and self.on_ai_turn_start:
            self.on_ai_turn_start()

        try:
            if is_ai_turn:
                # AI ê°„ ëŒ€í™”: ë§ˆì§€ë§‰ ë°œì–¸ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë°œì–¸ì ê²°ì •
                if self.last_ai_speaker == "AI1_friend":
                    next_speaker = "AI2_tutor"
                else:
                    next_speaker = "AI1_friend"
                logger.info(f"ğŸ¤–ğŸ—£ï¸ AI í„´ ì „í™˜: {self.last_ai_speaker} -> {next_speaker}")
            else:
                # ì‚¬ìš©ì í„´: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í˜¸ì¶œ
                logger.info(f"ğŸ—£ï¸ğŸ§ ğŸš€ [Gen {new_gen_id}] ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í˜¸ì¶œ ì¤‘...")
                formatted_history = "\n".join([f"{h['role']}: {h['content']}" for h in self.history])
                orchestrator_input = {"question": txt, "history": formatted_history}
                
                next_speaker = self.orchestrator_chain.invoke(orchestrator_input)
                logger.info(f"ğŸ¤–ğŸ—£ï¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ê²°ì •: {next_speaker}")

            # It's possible that the generation was aborted while waiting for the orchestrator.
            # Check if self.running_generation is still valid and for the current generation id.
            if not self.is_valid_gen() or self.running_generation.id != new_gen_id:
                logger.warning(f"ğŸ—£ï¸ğŸ”„ [Gen {new_gen_id}] ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í˜¸ì¶œ í›„ ìƒì„±ì´ ì¤‘ë‹¨ë˜ê±°ë‚˜ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ìƒì„±ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return

            # ê²°ì •ëœ ìŠ¤í”¼ì»¤ì— ë”°ë¼ ì²´ì¸ê³¼ ëª©ì†Œë¦¬ ì„ íƒ
            if "AI1_friend" in next_speaker:
                active_chain = self.rag_chain_ai1
                voice_id = "9BWtsMINqrJLrRacOk9x" # Aria
                # voice_id = "uyVNoMrnUku1dZyVEXwD" # Anna kim (í…ŒìŠ¤íŠ¸ìš© í•œêµ­ì–´ ëª¨ë¸)
                voice = "mia"   # ì˜¤ë¥´í˜ìš°ìŠ¤ ëª¨ë¸ìš©
                self.running_generation.speaker_role = "AI1_friend"
                self.last_ai_speaker = "AI1_friend"
            elif "AI2_tutor" in next_speaker:
                active_chain = self.rag_chain_ai2
                voice_id = "JBFqnCBsd6RMkjVDRZzb" # George
                # voice_id = "jB1Cifc2UQbq1gR3wnb0" # Bin (í…ŒìŠ¤íŠ¸ìš© í•œêµ­ì–´ ëª¨ë¸)
                voice = "leo"   # ì˜¤ë¥´í˜ìš°ìŠ¤ ëª¨ë¸ìš©
                self.running_generation.speaker_role = "AI2_tutor"
                self.last_ai_speaker = "AI2_tutor"
            else:
                logger.warning(f"ğŸ¤–â“ ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤í”¼ì»¤ ê²°ì •: {next_speaker}. ê¸°ë³¸ê°’(AI1) ì‚¬ìš©.")
                active_chain = self.rag_chain_ai1
                voice_id = "JBFqnCBsd6RMkjVDRZzb" # George
                voice = "mia"   # ì˜¤ë¥´í˜ìš°ìŠ¤ ëª¨ë¸ìš©
                self.running_generation.speaker_role = "AI1_friend"
                self.last_ai_speaker = "AI1_friend"

            if self.tts_engine == "elabs":
                self.audio.set_voice(voice_id)
                logger.info(f"ğŸ¤ğŸ”Š ElevenLabs ëª©ì†Œë¦¬ ë³€ê²½: {voice_id}")
            elif self.tts_engine == "orpheus":
                self.audio.set_voice(voice)
                logger.info(f"ğŸ¤ğŸ”Š ElevenLabs ëª©ì†Œë¦¬ ë³€ê²½: {voice}")
            else:
                # Coquiì™€ ê°™ì€ ë‹¤ë¥¸ ì—”ì§„ì„ ìœ„í•œ ê¸°ì¡´ ë¡œì§
                voice_path = "./voices/coqui_Daisy Studious.wav" if "AI1_friend" in next_speaker else "./voices/thsama.wav"
                self.audio.set_voice(voice_path)
                logger.info(f"ğŸ¤ğŸ”Š ëª©ì†Œë¦¬ ë³€ê²½: {voice_path}")

            formatted_history = "\n".join([f"{h['role']}: {h['content']}" for h in self.history])
            input_data = {"question": txt, "history": formatted_history}
            self.running_generation.llm_generator = active_chain.stream(input_data)
            
            logger.info(f"ğŸ—£ï¸ğŸ§ âœ”ï¸ [Gen {new_gen_id}] LLM ìƒì„±ê¸° ìƒì„±ë¨. ìƒì„±ê¸° ì¤€ë¹„ ì´ë²¤íŠ¸ ì„¤ì • ì¤‘.")
            self.generator_ready_event.set()
        except Exception as e:
            logger.exception(f"ğŸ—£ï¸ğŸ§ ğŸ’¥ [Gen {new_gen_id}] LLM ìƒì„±ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
            self.running_generation = None

    # ... (ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ì´ì „ê³¼ ê±°ì˜ ë™ì¼í•˜ê²Œ ìœ ì§€)
    def is_valid_gen(self) -> bool:
        return self.running_generation is not None and not self.running_generation.abortion_started

    def _request_processing_worker(self):
        logger.info("ğŸ—£ï¸ğŸš€ ìš”ì²­ ì²˜ë¦¬ê¸°: ì‹œì‘ ì¤‘...")
        while not self.shutdown_event.is_set():
            try:
                request = self.requests_queue.get(block=True, timeout=1)
                if self.previous_request:
                    if self.previous_request.data == request.data and isinstance(request.data, str):
                        if request.timestamp - self.previous_request.timestamp < 2:
                            logger.info(f"ğŸ—£ï¸ğŸ—‘ï¸ ìš”ì²­ ì²˜ë¦¬ê¸°: ì¤‘ë³µ ìš”ì²­ ê±´ë„ˆë›°ê¸° - {request.action}")
                            continue
                while not self.requests_queue.empty():
                    skipped_request = self.requests_queue.get(False)
                    logger.debug(f"ğŸ—£ï¸ğŸ—‘ï¸ ìš”ì²­ ì²˜ë¦¬ê¸°: ì˜¤ë˜ëœ ìš”ì²­ ê±´ë„ˆë›°ê¸° - {skipped_request.action}")
                    request = skipped_request
                self.abort_block_event.wait()
                logger.debug(f"ğŸ—£ï¸ğŸ”„ ìš”ì²­ ì²˜ë¦¬ê¸°: ê°€ì¥ ìµœê·¼ ìš”ì²­ ì²˜ë¦¬ ì¤‘ - {request.action}")
                if request.action == "prepare":
                    self.process_prepare_generation(request.data)
                    self.previous_request = request
                elif request.action == "finish":
                     logger.info(f"ğŸ—£ï¸ğŸ¤· ìš”ì²­ ì²˜ë¦¬ê¸°: 'finish' ì‘ì—… ìˆ˜ì‹  (í˜„ì¬ ì•„ë¬´ ì‘ì—… ì•ˆ í•¨).")
                     self.previous_request = request
                else:
                    logger.warning(f"ğŸ—£ï¸â“ ìš”ì²­ ì²˜ë¦¬ê¸°: ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—… '{request.action}'")
            except Empty:
                continue
            except Exception as e:
                logger.exception(f"ğŸ—£ï¸ğŸ’¥ ìš”ì²­ ì²˜ë¦¬ê¸°: ì˜¤ë¥˜: {e}")
        logger.info("ğŸ—£ï¸ğŸ ìš”ì²­ ì²˜ë¦¬ê¸°: ì¢…ë£Œ ì¤‘.")

    def on_first_audio_chunk_synthesize(self):
        logger.info("ğŸ—£ï¸ğŸ¶ ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤ ì²­í¬ê°€ í•©ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. TTS ë¹ ë¥¸ ë‹µë³€ í—ˆìš© ì´ë²¤íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.")
        if self.running_generation:
            self.running_generation.quick_answer_first_chunk_ready = True

    def preprocess_chunk(self, chunk: str) -> str:
        if isinstance(chunk, dict):
            chunk = chunk.get('answer', '')
        return chunk.replace("â€”", "-").replace("â€œ", '"').replace("â€", '"').replace("â€˜", "'").replace("â€™", "'").replace("â€¦", "...")

    def clean_quick_answer(self, text: str) -> str:
        patterns_to_remove = ["<think>", "</think>", "\n", " "]
        previous_text = None
        current_text = text
        while previous_text != current_text:
            previous_text = current_text
            for pattern in patterns_to_remove:
                while current_text.startswith(pattern):
                    current_text = current_text[len(pattern):]
        return current_text

    def _llm_inference_worker(self):
        logger.info("ğŸ—£ï¸ğŸ§  LLM ì›Œì»¤: ì‹œì‘ ì¤‘...")
        while not self.shutdown_event.is_set():
            ready = self.generator_ready_event.wait(timeout=1.0)
            if not ready:
                continue
            if self.stop_llm_request_event.is_set():
                logger.info("ğŸ—£ï¸ğŸ§ âŒ LLM ì›Œì»¤: generator_ready_event ëŒ€ê¸° ì¤‘ ì¤‘ë‹¨ ê°ì§€ë¨.")
                self.stop_llm_request_event.clear()
                self.stop_llm_finished_event.set()
                self.llm_generation_active = False
                continue
            self.generator_ready_event.clear()
            self.stop_everything_event.clear()
            current_gen = self.running_generation
            if not current_gen or not current_gen.llm_generator:
                logger.warning("ğŸ—£ï¸ğŸ§ â“ LLM ì›Œì»¤: ì´ë²¤íŠ¸ í›„ ìœ íš¨í•œ ìƒì„± ë˜ëŠ” ìƒì„±ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                self.llm_generation_active = False
                continue
            gen_id = current_gen.id
            logger.info(f"ğŸ—£ï¸ğŸ§ ğŸ”„ [Gen {gen_id}] LLM ì›Œì»¤: ìƒì„± ì²˜ë¦¬ ì¤‘...")
            self.llm_generation_active = True
            self.stop_llm_finished_event.clear()
            start_time = time.time()
            token_count = 0
            try:
                for chunk in current_gen.llm_generator:
                    if self.stop_llm_request_event.is_set():
                        logger.info(f"ğŸ—£ï¸ğŸ§ âŒ [Gen {gen_id}] LLM ì›Œì»¤: ë°˜ë³µ ì¤‘ ì¤‘ì§€ ìš”ì²­ ê°ì§€ë¨.")
                        self.stop_llm_request_event.clear()
                        current_gen.llm_aborted = True
                        break
                    
                    processed_chunk = self.preprocess_chunk(chunk)
                    token_count += 1
                    current_gen.quick_answer += processed_chunk

                    if self.no_think:
                        current_gen.quick_answer = self.clean_quick_answer(current_gen.quick_answer)
                    if token_count == 1:
                        logger.info(f"ğŸ—£ï¸ğŸ§ â±ï¸ [Gen {gen_id}] LLM ì›Œì»¤: TTFT: {(time.time() - start_time):.4f}s")
                    if not current_gen.quick_answer_provided:
                        context, overhang = self.text_context.get_context(current_gen.quick_answer)
                        if context:
                            logger.info(f"ğŸ—£ï¸ğŸ§ âœ”ï¸ [Gen {gen_id}] LLM ì›Œì»¤:  {Colors.apply('ë¹ ë¥¸ ë‹µë³€ ë°œê²¬:').magenta} {context}, ë‚˜ë¨¸ì§€: {overhang}")
                            current_gen.quick_answer = context
                            if self.on_partial_assistant_text:
                                self.on_partial_assistant_text(current_gen.quick_answer)
                            current_gen.quick_answer_overhang = overhang
                            current_gen.quick_answer_provided = True
                            self.llm_answer_ready_event.set()
                            break
                logger.info(f"ğŸ—£ï¸ğŸ§ ğŸ [Gen {gen_id}] LLM ì›Œì»¤: ìƒì„±ê¸° ë£¨í”„ ì™„ë£Œ%s" % (" (ì¤‘ë‹¨ë¨)" if current_gen.llm_aborted else ""))
                if not current_gen.llm_aborted and not current_gen.quick_answer_provided:
                    logger.info(f"ğŸ—£ï¸ğŸ§ âœ”ï¸ [Gen {gen_id}] LLM ì›Œì»¤: ì»¨í…ìŠ¤íŠ¸ ê²½ê³„ë¥¼ ì°¾ì§€ ëª»í•´ ì „ì²´ ì‘ë‹µì„ ë¹ ë¥¸ ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    current_gen.quick_answer_provided = True
                    if self.on_partial_assistant_text:
                        self.on_partial_assistant_text(current_gen.quick_answer)
                    self.llm_answer_ready_event.set()
            except Exception as e:
                logger.exception(f"ğŸ—£ï¸ğŸ§ ğŸ’¥ [Gen {gen_id}] LLM ì›Œì»¤: ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                current_gen.llm_aborted = True
            finally:
                self.llm_generation_active = False
                self.stop_llm_finished_event.set()
                if current_gen.llm_aborted:
                    logger.info(f"ğŸ—£ï¸ğŸ§ âŒ [Gen {gen_id}] LLM ì¤‘ë‹¨ë¨, TTS ë¹ ë¥¸/ìµœì¢… ì¤‘ì§€ ìš”ì²­.")
                    self.stop_tts_quick_request_event.set()
                    self.stop_tts_final_request_event.set()
                    self.llm_answer_ready_event.set()
                logger.info(f"ğŸ—£ï¸ğŸ§ ğŸ [Gen {gen_id}] LLM ì›Œì»¤: ì²˜ë¦¬ ì£¼ê¸° ì™„ë£Œ.")
                current_gen.llm_finished = True
                current_gen.llm_finished_event.set()

    def check_abort(self, txt: str, wait_for_finish: bool = True, abort_reason: str = "unknown") -> bool:
        with self.check_abort_lock:
            if self.running_generation:
                current_gen_id_str = f"Gen {self.running_generation.id}"
                logger.info(f"ğŸ—£ï¸ğŸ›‘â“ {current_gen_id_str} ì¤‘ë‹¨ í™•ì¸ ìš”ì²­ë¨ (ì´ìœ : {abort_reason})")
                if self.running_generation.abortion_started:
                    logger.info(f"ğŸ—£ï¸ğŸ›‘â³ {current_gen_id_str} í™œì„± ìƒì„±ì´ ì´ë¯¸ ì¤‘ë‹¨ ì¤‘ì´ë¯€ë¡œ, ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤ (ìš”ì²­ëœ ê²½ìš°).")
                    if wait_for_finish:
                        completed = self.abort_completed_event.wait(timeout=5.0)
                        if not completed:
                             logger.error(f"ğŸ—£ï¸ğŸ›‘ğŸ’¥ğŸ’¥ {current_gen_id_str} ì§„í–‰ ì¤‘ì¸ ì¤‘ë‹¨ ì™„ë£Œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼. ìƒíƒœ ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„±!")
                             self.running_generation = None
                        elif self.running_generation is not None:
                            logger.error(f"ğŸ—£ï¸ğŸ›‘ğŸ’¥ğŸ’¥ {current_gen_id_str} ì¤‘ë‹¨ ì™„ë£Œ ì´ë²¤íŠ¸ê°€ ì„¤ì •ë˜ì—ˆì§€ë§Œ running_generationì´ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒíƒœ ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„± ë†’ìŒ!")
                            self.running_generation = None
                        else:
                            logger.info(f"ğŸ—£ï¸ğŸ›‘âœ… {current_gen_id_str} ì§„í–‰ ì¤‘ì¸ ì¤‘ë‹¨ ì™„ë£Œ.")
                    else:
                        logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸƒ {current_gen_id_str} wait_for_finish=Falseì´ë¯€ë¡œ ì§„í–‰ ì¤‘ì¸ ì¤‘ë‹¨ì„ ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return True
                else:
                    logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ¤” {current_gen_id_str} í™œì„± ìƒì„± ë°œê²¬, í…ìŠ¤íŠ¸ ìœ ì‚¬ì„± í™•ì¸ ì¤‘.")
                    try:
                        if self.running_generation.text is None:
                            logger.warning(f"ğŸ—£ï¸ğŸ›‘â“ {current_gen_id_str} ì‹¤í–‰ ì¤‘ì¸ ìƒì„± í…ìŠ¤íŠ¸ê°€ Noneì´ë¯€ë¡œ ìœ ì‚¬ì„±ì„ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥´ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.")
                            similarity = 0.0
                        else:
                            similarity = self.text_similarity.calculate_similarity(self.running_generation.text, txt)
                    except Exception as e:
                        logger.warning(f"ğŸ—£ï¸ğŸ›‘ğŸ’¥ {current_gen_id_str} ìœ ì‚¬ì„± ê³„ì‚° ì˜¤ë¥˜: {e}. ë‹¤ë¥´ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.")
                        similarity = 0.0
                    if similarity >= 0.95:
                        logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ™… {current_gen_id_str} í…ìŠ¤íŠ¸('{txt[:30]}...')ê°€ í˜„ì¬ '{self.running_generation.text[:30] if self.running_generation.text else 'None'}...'ì™€ ë„ˆë¬´ ìœ ì‚¬í•¨({similarity:.2f}). ë¬´ì‹œí•©ë‹ˆë‹¤.")
                        return False
                    logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸš€ {current_gen_id_str} í…ìŠ¤íŠ¸('{txt[:30]}...')ê°€ '{self.running_generation.text[:30] if self.running_generation.text else 'None'}...'ì™€ ì¶©ë¶„íˆ ë‹¤ë¦„({similarity:.2f}). ë™ê¸°ì  ì¤‘ë‹¨ ìš”ì²­.")
                    start_time = time.time()
                    self.abort_generation(wait_for_completion=wait_for_finish, timeout=7.0, reason=f"check_abortê°€ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ë°œê²¬ ({abort_reason})")
                    if wait_for_finish:
                        if self.running_generation is not None:
                            logger.error(f"ğŸ—£ï¸ğŸ›‘ğŸ’¥ğŸ’¥ {current_gen_id_str} !!! ì¤‘ë‹¨ í˜¸ì¶œì´ ì™„ë£Œë˜ì—ˆì§€ë§Œ running_generationì´ ì—¬ì „íˆ Noneì´ ì•„ë‹™ë‹ˆë‹¤. ìƒíƒœ ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„± ë†’ìŒ!")
                            self.running_generation = None
                        else:
                            logger.info(f"ğŸ—£ï¸ğŸ›‘âœ… {current_gen_id_str} ë™ê¸°ì  ì¤‘ë‹¨ì´ {time.time() - start_time:.2f}s ë‚´ì— ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    return True
            else:
                logger.info("ğŸ—£ï¸ğŸ›‘ğŸ¤· ì¤‘ë‹¨ í™•ì¸ ì¤‘ í™œì„± ìƒì„±ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False

    def _tts_quick_inference_worker(self):
        logger.info("ğŸ—£ï¸ğŸ‘„ğŸš€ ë¹ ë¥¸ TTS ì›Œì»¤: ì‹œì‘ ì¤‘...")
        while not self.shutdown_event.is_set():
            ready = self.llm_answer_ready_event.wait(timeout=1.0)
            if not ready:
                continue
            if self.stop_tts_quick_request_event.is_set():
                logger.info("ğŸ—£ï¸ğŸ‘„âŒ ë¹ ë¥¸ TTS ì›Œì»¤: llm_answer_ready_event ëŒ€ê¸° ì¤‘ ì¤‘ë‹¨ ê°ì§€ë¨.")
                self.stop_tts_quick_request_event.clear()
                self.stop_tts_quick_finished_event.set()
                self.tts_quick_generation_active = False
                continue
            self.llm_answer_ready_event.clear()
            current_gen = self.running_generation
            if not current_gen or not current_gen.quick_answer:
                logger.warning("ğŸ—£ï¸ğŸ‘„â“ ë¹ ë¥¸ TTS ì›Œì»¤: ì´ë²¤íŠ¸ í›„ ìœ íš¨í•œ ìƒì„± ë˜ëŠ” ë¹ ë¥¸ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                self.tts_quick_generation_active = False
                continue
            if current_gen.audio_quick_aborted or current_gen.abortion_started:
                logger.info(f"ğŸ—£ï¸ğŸ‘„âŒ [Gen {current_gen.id}] ë¹ ë¥¸ TTS ì›Œì»¤: ìƒì„±ì´ ì´ë¯¸ ì¤‘ë‹¨ë¨ìœ¼ë¡œ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            gen_id = current_gen.id
            logger.info(f"ğŸ—£ï¸ğŸ‘„ğŸ”„ [Gen {gen_id}] ë¹ ë¥¸ TTS ì›Œì»¤: ë¹ ë¥¸ ë‹µë³€ì— ëŒ€í•œ TTS ì²˜ë¦¬ ì¤‘...")
            self.tts_quick_generation_active = True
            self.stop_tts_quick_finished_event.clear()
            current_gen.tts_quick_finished_event.clear()
            current_gen.tts_quick_started = True
            allowed_to_speak = True
            try:
                if self.stop_tts_quick_request_event.is_set() or current_gen.abortion_started:
                     logger.info(f"ğŸ—£ï¸ğŸ‘„âŒ [Gen {gen_id}] ë¹ ë¥¸ TTS ì›Œì»¤: ì¤‘ì§€ ìš”ì²­ ë˜ëŠ” ì¤‘ë‹¨ í”Œë˜ê·¸ë¡œ ì¸í•´ TTS í•©ì„±ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                     current_gen.audio_quick_aborted = True
                else:
                    logger.info(f"ğŸ—£ï¸ğŸ‘„ğŸ¶ [Gen {gen_id}] ë¹ ë¥¸ TTS ì›Œì»¤: í•©ì„± ì¤‘: '{current_gen.quick_answer[:50]}...' ")
                    completed = self.audio.synthesize(
                        current_gen.quick_answer,
                        current_gen.audio_chunks,
                        self.stop_tts_quick_request_event,
                        generation_obj=current_gen
                    )
                    if not completed:
                        logger.info(f"ğŸ—£ï¸ğŸ‘„âŒ [Gen {gen_id}] ë¹ ë¥¸ TTS ì›Œì»¤: ì´ë²¤íŠ¸ë¡œ í•©ì„± ì¤‘ì§€ë¨.")
                        current_gen.audio_quick_aborted = True
                    else:
                        logger.info(f"ğŸ—£ï¸ğŸ‘„âœ… [Gen {gen_id}] ë¹ ë¥¸ TTS ì›Œì»¤: í•©ì„±ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.exception(f"ğŸ—£ï¸ğŸ‘„ğŸ’¥ [Gen {gen_id}] ë¹ ë¥¸ TTS ì›Œì»¤: í•©ì„± ì¤‘ ì˜¤ë¥˜: {e}")
                current_gen.audio_quick_aborted = True
            finally:
                self.tts_quick_generation_active = False
                self.stop_tts_quick_finished_event.set()
                logger.info(f"ğŸ—£ï¸ğŸ‘„ğŸ [Gen {gen_id}] ë¹ ë¥¸ TTS ì›Œì»¤: ì²˜ë¦¬ ì£¼ê¸° ì™„ë£Œ.")
                if current_gen.audio_quick_aborted or self.stop_tts_quick_request_event.is_set():
                    logger.info(f"ğŸ—£ï¸ğŸ‘„âŒ [Gen {gen_id}] ë¹ ë¥¸ TTSê°€ ì¤‘ë‹¨/ë¯¸ì™„ë£Œë¡œ í‘œì‹œë¨.")
                    self.stop_tts_quick_request_event.clear()
                    current_gen.audio_quick_aborted = True
                else:
                    logger.info(f"ğŸ—£ï¸ğŸ‘„âœ… [Gen {gen_id}] ë¹ ë¥¸ TTSê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨.")
                    current_gen.tts_quick_finished_event.set()
                current_gen.audio_quick_finished = True

    def _tts_final_inference_worker(self):
        logger.info("ğŸ—£ï¸ğŸ‘„ğŸš€ ìµœì¢… TTS ì›Œì»¤: ì‹œì‘ ì¤‘...")
        while not self.shutdown_event.is_set():
            current_gen = self.running_generation
            time.sleep(0.01)
            if not current_gen: continue
            if current_gen.tts_final_started: continue
            if not current_gen.tts_quick_started: continue
            if not current_gen.audio_quick_finished: continue
            gen_id = current_gen.id
            if current_gen.audio_quick_aborted:
                continue
            if not current_gen.quick_answer_provided:
                 logger.debug(f"ğŸ—£ï¸ğŸ‘„ğŸ™… [Gen {gen_id}] ìµœì¢… TTS ì›Œì»¤: ë¹ ë¥¸ ë‹µë³€ ê²½ê³„ë¥¼ ì°¾ì§€ ëª»í•´ ìµœì¢… TTSë¥¼ ê±´ë„ˆëœë‹ˆë‹¤ (ë¹ ë¥¸ TTSê°€ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬í•¨).")
                 continue
            if current_gen.abortion_started:
                 logger.debug(f"ğŸ—£ï¸ğŸ‘„ğŸ™… [Gen {gen_id}] ìµœì¢… TTS ì›Œì»¤: ìƒì„±ì´ ì¤‘ë‹¨ ì¤‘ì´ë¯€ë¡œ ìµœì¢… TTSë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                 continue
            logger.info(f"ğŸ—£ï¸ğŸ‘„ğŸ”„ [Gen {gen_id}] ìµœì¢… TTS ì›Œì»¤: ìµœì¢… TTS ì²˜ë¦¬ ì¤‘...")
            def get_generator():
                if current_gen.quick_answer_overhang:
                    preprocessed_overhang = self.preprocess_chunk(current_gen.quick_answer_overhang)
                    logger.debug(f"ğŸ—£ï¸ğŸ‘„< [Gen {gen_id}] ìµœì¢… TTS ìƒì„±ê¸°: ë‚˜ë¨¸ì§€ ë¶€ë¶„ ìƒì„± ì¤‘: '{preprocessed_overhang[:50]}...' ")
                    current_gen.final_answer += preprocessed_overhang
                    if self.on_partial_assistant_text:
                         logger.debug(f"ğŸ—£ï¸ğŸ‘„< [Gen {gen_id}] ìµœì¢… TTS ì›Œì»¤ on_partial_assistant_text: ë‚˜ë¨¸ì§€ ë¶€ë¶„ ì „ì†¡ ì¤‘.")
                         try:
                            self.on_partial_assistant_text(current_gen.quick_answer + current_gen.final_answer)
                         except Exception as cb_e:
                             logger.warning(f"ğŸ—£ï¸ğŸ’¥ on_partial_assistant_text ì½œë°± ì˜¤ë¥˜ (ë‚˜ë¨¸ì§€ ë¶€ë¶„): {cb_e}")
                    yield preprocessed_overhang
                logger.debug(f"ğŸ—£ï¸ğŸ‘„< [Gen {gen_id}] ìµœì¢… TTS ìƒì„±ê¸°: ë‚¨ì€ LLM ì²­í¬ ìƒì„± ì¤‘...")
                try:
                    for chunk in current_gen.llm_generator:
                         if self.stop_tts_final_request_event.is_set():
                             logger.info(f"ğŸ—£ï¸ğŸ‘„âŒ [Gen {gen_id}] ìµœì¢… TTS ìƒì„±ê¸°: LLM ë°˜ë³µ ì¤‘ ì¤‘ì§€ ìš”ì²­ ê°ì§€ë¨.")
                             current_gen.audio_final_aborted = True
                             break
                         preprocessed_chunk = self.preprocess_chunk(chunk)
                         current_gen.final_answer += preprocessed_chunk
                         if self.on_partial_assistant_text:
                            try:
                                 self.on_partial_assistant_text(current_gen.quick_answer + current_gen.final_answer)
                            except Exception as cb_e:
                                 logger.warning(f"ğŸ—£ï¸ğŸ’¥ on_partial_assistant_text ì½œë°± ì˜¤ë¥˜ (ìµœì¢… ì²­í¬): {cb_e}")
                         yield preprocessed_chunk
                    logger.debug(f"ğŸ—£ï¸ğŸ‘„< [Gen {gen_id}] ìµœì¢… TTS ìƒì„±ê¸°: LLM ì²­í¬ ë°˜ë³µ ì™„ë£Œ.")
                except Exception as gen_e:
                     logger.exception(f"ğŸ—£ï¸ğŸ‘„ğŸ’¥ [Gen {gen_id}] ìµœì¢… TTS ìƒì„±ê¸°: LLM ìƒì„±ê¸° ë°˜ë³µ ì¤‘ ì˜¤ë¥˜: {gen_e}")
                     current_gen.audio_final_aborted = True
            self.tts_final_generation_active = True
            self.stop_tts_final_finished_event.clear()
            current_gen.tts_final_started = True
            current_gen.tts_final_finished_event.clear()
            try:
                logger.info(f"ğŸ—£ï¸ğŸ‘„ğŸ¶ [Gen {gen_id}] ìµœì¢… TTS ì›Œì»¤: ë‚¨ì€ í…ìŠ¤íŠ¸ í•©ì„± ì¤‘...")
                completed = self.audio.synthesize_generator(
                    get_generator(),
                    current_gen.audio_chunks,
                    self.stop_tts_final_request_event,
                    generation_obj=current_gen
                )
                if not completed:
                     logger.info(f"ğŸ—£ï¸ğŸ‘„âŒ [Gen {gen_id}] ìµœì¢… TTS ì›Œì»¤: ì´ë²¤íŠ¸ë¡œ í•©ì„± ì¤‘ì§€ë¨.")
                     current_gen.audio_final_aborted = True
                else:
                    logger.info(f"ğŸ—£ï¸ğŸ‘„âœ… [Gen {gen_id}] ìµœì¢… TTS ì›Œì»¤: í•©ì„±ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.exception(f"ğŸ—£ï¸ğŸ‘„ğŸ’¥ [Gen {gen_id}] ìµœì¢… TTS ì›Œì»¤: í•©ì„± ì¤‘ ì˜¤ë¥˜: {e}")
                current_gen.audio_final_aborted = True
            finally:
                self.tts_final_generation_active = False
                self.stop_tts_final_finished_event.set()
                logger.info(f"ğŸ—£ï¸ğŸ‘„ğŸ [Gen {gen_id}] ìµœì¢… TTS ì›Œì»¤: ì²˜ë¦¬ ì£¼ê¸° ì™„ë£Œ.")
                if current_gen.audio_final_aborted or self.stop_tts_final_request_event.is_set():
                    logger.info(f"ğŸ—£ï¸ğŸ‘„âŒ [Gen {gen_id}] ìµœì¢… TTSê°€ ì¤‘ë‹¨/ë¯¸ì™„ë£Œë¡œ í‘œì‹œë¨.")
                    self.stop_tts_final_request_event.clear()
                    current_gen.audio_final_aborted = True
                else:
                    logger.info(f"ğŸ—£ï¸ğŸ‘„âœ… [Gen {gen_id}] ìµœì¢… TTSê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨.")
                    current_gen.tts_final_finished_event.set()
                    # ëª¨ë“  TTS ì™„ë£Œ ì‹œ ì½œë°± í˜¸ì¶œ
                    if hasattr(self, 'on_tts_all_complete') and self.on_tts_all_complete:
                        try:
                            self.on_tts_all_complete()
                        except Exception as e:
                            logger.error(f"ğŸ—£ï¸ğŸ‘„ğŸ’¥ TTS ì™„ë£Œ ì½œë°± ì˜¤ë¥˜: {e}")
                current_gen.audio_final_finished = True

    def prepare_generation(self, txt: str, is_ai_turn: bool = False):
        logger.info(f"ğŸ—£ï¸ğŸ“¥ 'prepare' ìš”ì²­ ëŒ€ê¸°ì—´ì— ì¶”ê°€ ì¤‘: '{txt[:50]}...' (AI í„´: {is_ai_turn})")
        self.requests_queue.put(PipelineRequest("prepare", {"text": txt, "is_ai_turn": is_ai_turn}))

    def finish_generation(self):
        logger.info(f"ğŸ—£ï¸ğŸ“¥ 'finish' ìš”ì²­ ëŒ€ê¸°ì—´ì— ì¶”ê°€ ì¤‘")
        self.requests_queue.put(PipelineRequest("finish"))

    def abort_generation(self, wait_for_completion: bool = False, timeout: float = 7.0, reason: str = ""):
        if self.shutdown_event.is_set():
            logger.warning("ğŸ—£ï¸ğŸ”Œ ì¢…ë£Œ ì§„í–‰ ì¤‘, ì¤‘ë‹¨ ìš”ì²­ ë¬´ì‹œ.")
            return
        gen_id_str = f"Gen {self.running_generation.id}" if self.running_generation else "Gen None"
        logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸš€ 'abort' ìš”ì²­ ì¤‘ (wait={wait_for_completion}, reason='{reason}') for {gen_id_str}")
        self.process_abort_generation()
        if wait_for_completion:
            logger.info(f"ğŸ—£ï¸ğŸ›‘â³ ì¤‘ë‹¨ ì™„ë£Œ ëŒ€ê¸° ì¤‘ (íƒ€ì„ì•„ì›ƒ={timeout}s)...")
            completed = self.abort_completed_event.wait(timeout=timeout)
            if completed:
                logger.info(f"ğŸ—£ï¸ğŸ›‘âœ… ì¤‘ë‹¨ ì™„ë£Œ í™•ì¸ë¨.")
            else:
                logger.warning(f"ğŸ—£ï¸ğŸ›‘â±ï¸ ì¤‘ë‹¨ ì™„ë£Œ ì´ë²¤íŠ¸ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼.")
            self.abort_block_event.set()

    def process_abort_generation(self):
        with self.abort_lock:
            current_gen_obj = self.running_generation
            current_gen_id_str = f"Gen {current_gen_obj.id}" if current_gen_obj else "Gen None"
            if current_gen_obj is None or current_gen_obj.abortion_started:
                if current_gen_obj is None:
                    logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ¤· {current_gen_id_str} ì¤‘ë‹¨í•  í™œì„± ìƒì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    logger.info(f"ğŸ—£ï¸ğŸ›‘â³ {current_gen_id_str} ì¤‘ë‹¨ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.")
                self.abort_completed_event.set()
                self.abort_block_event.set()
                return
            logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸš€ {current_gen_id_str} ì¤‘ë‹¨ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì¤‘...")
            current_gen_obj.abortion_started = True
            self.abort_block_event.clear()
            self.abort_completed_event.clear()
            self.stop_everything_event.set()
            aborted_something = False
            is_llm_potentially_active = self.llm_generation_active or self.generator_ready_event.is_set()
            if is_llm_potentially_active:
                logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ§ âŒ {current_gen_id_str} - LLM ì¤‘ì§€ ì¤‘...")
                self.stop_llm_request_event.set()
                self.generator_ready_event.set()
                stopped = self.stop_llm_finished_event.wait(timeout=5.0)
                if stopped:
                    logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ§ ğŸ‘ {current_gen_id_str} LLM ì¤‘ì§€ í™•ì¸ ìˆ˜ì‹ ë¨.")
                    self.stop_llm_finished_event.clear()
                else:
                    logger.warning(f"ğŸ—£ï¸ğŸ›‘ğŸ§ â±ï¸ {current_gen_id_str} LLM ì¤‘ì§€ í™•ì¸ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼.")
                if hasattr(self.llm, 'cancel_generation'):
                    logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ§ ğŸ”Œ {current_gen_id_str} ì™¸ë¶€ LLM cancel_generation í˜¸ì¶œ ì¤‘.")
                    try:
                        self.llm.cancel_generation()
                    except Exception as cancel_e:
                         logger.warning(f"ğŸ—£ï¸ğŸ›‘ğŸ§ ğŸ’¥ {current_gen_id_str} ì™¸ë¶€ LLM ì·¨ì†Œ ì¤‘ ì˜¤ë¥˜: {cancel_e}")
                self.llm_generation_active = False
                aborted_something = True
            else:
                logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ§ ğŸ“´ {current_gen_id_str} LLMì´ ë¹„í™œì„± ìƒíƒœë¡œ ë³´ì—¬ ì¤‘ì§€ í•„ìš” ì—†ìŒ.")
            self.stop_llm_request_event.clear()
            is_tts_quick_potentially_active = self.tts_quick_generation_active or self.llm_answer_ready_event.is_set()
            if is_tts_quick_potentially_active:
                logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ‘„âŒ {current_gen_id_str} ë¹ ë¥¸ TTS ì¤‘ì§€ ì¤‘...")
                self.stop_tts_quick_request_event.set()
                self.llm_answer_ready_event.set()
                stopped = self.stop_tts_quick_finished_event.wait(timeout=5.0)
                if stopped:
                    logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ‘„ğŸ‘ {current_gen_id_str} ë¹ ë¥¸ TTS ì¤‘ì§€ í™•ì¸ ìˆ˜ì‹ ë¨.")
                    self.stop_tts_quick_finished_event.clear()
                else:
                    logger.warning(f"ğŸ—£ï¸ğŸ›‘ğŸ‘„â±ï¸ {current_gen_id_str} ë¹ ë¥¸ TTS ì¤‘ì§€ í™•ì¸ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼.")
                self.tts_quick_generation_active = False
                aborted_something = True
            else:
                logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ‘„ğŸ“´ {current_gen_id_str} ë¹ ë¥¸ TTSê°€ ë¹„í™œì„± ìƒíƒœë¡œ ë³´ì—¬ ì¤‘ì§€ í•„ìš” ì—†ìŒ.")
            self.stop_tts_quick_request_event.clear()
            is_tts_final_potentially_active = self.tts_final_generation_active
            if is_tts_final_potentially_active:
                logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ‘„âŒ {current_gen_id_str} ìµœì¢… TTS ì¤‘ì§€ ì¤‘...")
                self.stop_tts_final_request_event.set()
                stopped = self.stop_tts_final_finished_event.wait(timeout=5.0)
                if stopped:
                    logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ‘„ğŸ‘ {current_gen_id_str} ìµœì¢… TTS ì¤‘ì§€ í™•ì¸ ìˆ˜ì‹ ë¨.")
                    self.stop_tts_final_finished_event.clear()
                else:
                    logger.warning(f"ğŸ—£ï¸ğŸ›‘ğŸ‘„â±ï¸ {current_gen_id_str} ìµœì¢… TTS ì¤‘ì§€ í™•ì¸ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼.")
                self.tts_final_generation_active = False
                aborted_something = True
            else:
                logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ‘„ğŸ“´ {current_gen_id_str} ìµœì¢… TTSê°€ ë¹„í™œì„± ìƒíƒœë¡œ ë³´ì—¬ ì¤‘ì§€ í•„ìš” ì—†ìŒ.")
            self.stop_tts_final_request_event.clear()
            if hasattr(self.audio, 'stop_playback'):
                logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ”Š {current_gen_id_str} ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘ì§€ ìš”ì²­ ì¤‘.")
                try:
                    self.audio.stop_playback()
                except Exception as audio_e:
                    logger.warning(f"ğŸ—£ï¸ğŸ›‘ğŸ”ŠğŸ’¥ {current_gen_id_str} ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘ì§€ ì˜¤ë¥˜: {audio_e}")
            if self.running_generation is not None and self.running_generation.id == current_gen_obj.id:
                logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ§¹ {current_gen_id_str} ì‹¤í–‰ ì¤‘ì¸ ìƒì„± ê°ì²´ ì§€ìš°ëŠ” ì¤‘.")
                if current_gen_obj.llm_generator and hasattr(current_gen_obj.llm_generator, 'close'):
                    try:
                        logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ§ ğŸ”Œ {current_gen_id_str} LLM ìƒì„±ê¸° ìŠ¤íŠ¸ë¦¼ ë‹«ëŠ” ì¤‘.")
                        current_gen_obj.llm_generator.close()
                    except Exception as e:
                        logger.warning(f"ğŸ—£ï¸ğŸ›‘ğŸ§ ğŸ’¥ {current_gen_id_str} LLM ìƒì„±ê¸° ë‹«ê¸° ì˜¤ë¥˜: {e}")
                self.running_generation = None
            elif self.running_generation is not None and self.running_generation.id != current_gen_obj.id:
                 logger.warning(f"ğŸ—£ï¸ğŸ›‘â“ {current_gen_id_str} ë¶ˆì¼ì¹˜: ì¤‘ë‹¨ ì¤‘ self.running_generationì´ ë³€ê²½ë¨ (í˜„ì¬ Gen {self.running_generation.id}). í˜„ì¬ ì°¸ì¡° ì§€ìš°ëŠ” ì¤‘.")
                 self.running_generation = None
            elif aborted_something:
                logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ¤· {current_gen_id_str} ì›Œì»¤ê°€ ì¤‘ë‹¨ë˜ì—ˆì§€ë§Œ running_generationì€ ì´ë¯¸ Noneì´ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logger.info(f"ğŸ—£ï¸ğŸ›‘ğŸ¤· {current_gen_id_str} ì¤‘ë‹¨í•  í™œì„± í•­ëª©ì´ ì—†ëŠ” ê²ƒ ê°™ê³ , running_generationì€ Noneì…ë‹ˆë‹¤.")
            self.generator_ready_event.clear()
            self.llm_answer_ready_event.clear()
            logger.info(f"ğŸ—£ï¸ğŸ›‘âœ… {current_gen_id_str} ì¤‘ë‹¨ ì²˜ë¦¬ ì™„ë£Œ. ì™„ë£Œ ì´ë²¤íŠ¸ ì„¤ì • ë° ë¸”ë¡ í•´ì œ ì¤‘.")
            self.abort_completed_event.set()
            self.abort_block_event.set()

    def reset(self):
        logger.info("ğŸ—£ï¸ğŸ”„ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë¦¬ì…‹ ì¤‘...")
        self.ai_to_ai_active = False
        self.abort_generation(wait_for_completion=True, timeout=7.0, reason="reset")
        self.history = []
        logger.info("ğŸ—£ï¸ğŸ§¹ ê¸°ë¡ì´ ì§€ì›Œì¡ŒìŠµë‹ˆë‹¤. ë¦¬ì…‹ ì™„ë£Œ.")

    def shutdown(self):
        logger.info("ğŸ—£ï¸ğŸ”Œ ì¢…ë£Œ ì‹œì‘ ì¤‘...")
        self.shutdown_event.set()
        logger.info("ğŸ—£ï¸ğŸ”ŒğŸ›‘ ìŠ¤ë ˆë“œ ì¡°ì¸ ì „ ìµœì¢… ì¤‘ë‹¨ ì‹œë„ ì¤‘...")
        self.abort_generation(wait_for_completion=True, timeout=3.0, reason="shutdown")
        logger.info("ğŸ—£ï¸ğŸ”ŒğŸ”” ëŒ€ê¸° ì¤‘ì¸ ìŠ¤ë ˆë“œë¥¼ ê¹¨ìš°ê¸° ìœ„í•´ ì´ë²¤íŠ¸ ì‹ í˜¸ ì¤‘...")
        self.generator_ready_event.set()
        self.llm_answer_ready_event.set()
        self.stop_llm_finished_event.set()
        self.stop_tts_quick_finished_event.set()
        self.stop_tts_final_finished_event.set()
        self.abort_completed_event.set()
        self.abort_block_event.set()
        threads_to_join = [
            (self.request_processing_thread, "ìš”ì²­ ì²˜ë¦¬ê¸°"),
            (self.llm_inference_thread, "LLM ì›Œì»¤"),
            (self.tts_quick_inference_thread, "ë¹ ë¥¸ TTS ì›Œì»¤"),
            (self.tts_final_inference_thread, "ìµœì¢… TTS ì›Œì»¤"),
        ]
        for thread, name in threads_to_join:
             if thread.is_alive():
                 logger.info(f"ğŸ—£ï¸ğŸ”Œâ³ {name} ì¡°ì¸ ì¤‘...")
                 thread.join(timeout=5.0)
                 if thread.is_alive():
                     logger.warning(f"ğŸ—£ï¸ğŸ”Œâ±ï¸ {name} ìŠ¤ë ˆë“œê°€ ê¹¨ë—í•˜ê²Œ ì¡°ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
             else:
                  logger.info(f"ğŸ—£ï¸ğŸ”ŒğŸ‘ {name} ìŠ¤ë ˆë“œê°€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info("ğŸ—£ï¸ğŸ”Œâœ… ì¢…ë£Œ ì™„ë£Œ.")
        
        # AudioProcessor ì •ë¦¬
        if hasattr(self, 'audio_processor') and self.audio_processor:
            try:
                logger.info("ğŸ—£ï¸ğŸ§¹ AudioProcessor ì •ë¦¬ ì¤‘...")
                self.audio_processor.cleanup()
            except Exception as e:
                logger.error(f"ğŸ—£ï¸ğŸ’¥ AudioProcessor ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)