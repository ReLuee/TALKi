import logging
logger = logging.getLogger(__name__)

import transformers
import collections
import threading
import queue
import torch
import time
import re

# ì„¤ì • ìƒìˆ˜
model_dir_local = "KoljaB/SentenceFinishedClassification"
model_dir_cloud = "/root/models/sentenceclassification/"
sentence_end_marks = ['.', '!', '?', 'ã€‚'] # ë¬¸ì¥ ëìœ¼ë¡œ ê°„ì£¼ë˜ëŠ” ë¬¸ìë“¤

# í™•ë¥ -ì •ì§€ ì‹œê°„ ë³´ê°„ì„ ìœ„í•œ ì•µì»¤ í¬ì¸íŠ¸
anchor_points = [
    (0.0, 1.0), # í™•ë¥  0.0ì€ ì •ì§€ ì‹œê°„ 1.0ì— ë§¤í•‘
    (1.0, 0.0)  # í™•ë¥  1.0ì€ ì •ì§€ ì‹œê°„ 0.0ì— ë§¤í•‘
]

def ends_with_string(text: str, s: str) -> bool:
    """
    ë¬¸ìì—´ì´ íŠ¹ì • ë¶€ë¶„ ë¬¸ìì—´ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸í•˜ë©°, ë’¤ì— í•œ ê¸€ì ì •ë„ì˜ ì—¬ìœ ë¥¼ í—ˆìš©í•©ë‹ˆë‹¤.

    ë’¤ì— ê³µë°±ì´ ìˆëŠ” ê²½ìš°ì—ë„ êµ¬ë‘ì ì„ í™•ì¸í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.

    Args:
        text: í™•ì¸í•  ì…ë ¥ ë¬¸ìì—´.
        s: ëì—ì„œ ì°¾ì„ ë¶€ë¶„ ë¬¸ìì—´.

    Returns:
        í…ìŠ¤íŠ¸ê°€ së¡œ ëë‚˜ê±°ë‚˜, text[:-1]ì´ së¡œ ëë‚˜ë©´ True. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False.
    """
    if text.endswith(s):
        return True
    # ë§ˆì§€ë§‰ ë¬¸ìë¥¼ ë¬´ì‹œí–ˆì„ ë•Œ ëë‚˜ëŠ”ì§€ í™•ì¸ (ì˜ˆ: ë’¤ë”°ë¥´ëŠ” ê³µë°±)
    if len(text) > 1 and text[:-1].endswith(s):
        return True
    return False

def preprocess_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ë¬¸ìì—´ì˜ ì‹œì‘ ë¶€ë¶„ì„ ì •ë¦¬í•˜ê³  ì •ê·œí™”í•©ë‹ˆë‹¤.

    ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì ìš©í•©ë‹ˆë‹¤:
    1. ì•ìª½ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.
    2. ì•ìª½ ë§ì¤„ì„í‘œ("...")ê°€ ìˆìœ¼ë©´ ì œê±°í•©ë‹ˆë‹¤.
    3. (ë§ì¤„ì„í‘œ ì œê±° í›„) ë‹¤ì‹œ ì•ìª½ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.
    4. ë‚¨ì€ í…ìŠ¤íŠ¸ì˜ ì²« ê¸€ìë¥¼ ëŒ€ë¬¸ìë¡œ ë§Œë“­ë‹ˆë‹¤.

    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸ ë¬¸ìì—´.

    Returns:
        ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë¬¸ìì—´.
    """
    text = text.lstrip() # ì•ìª½ ê³µë°± ì œê±°
    if text.startswith("..."): # ì‹œì‘í•˜ëŠ” ë§ì¤„ì„í‘œê°€ ìˆìœ¼ë©´ ì œê±°
        text = text[3:]
    text = text.lstrip() # ë§ì¤„ì„í‘œ ì œê±° í›„ ë‹¤ì‹œ ì•ìª½ ê³µë°± ì œê±°
    if text:
        text = text[0].upper() + text[1:] # ì²« ê¸€ì ëŒ€ë¬¸ìí™”

    return text

def strip_ending_punctuation(text: str) -> str:
    """
    `sentence_end_marks`ì— ì •ì˜ëœ ë’¤ìª½ êµ¬ë‘ì ì„ ì œê±°í•©ë‹ˆë‹¤.

    ë¨¼ì € ë’¤ìª½ ê³µë°±ì„ ì œê±°í•œ ë‹¤ìŒ, ë¬¸ìì—´ ëì—ì„œ ë°œê²¬ë˜ëŠ” `sentence_end_marks`ì˜
    ë¬¸ìë“¤ì„ ë°˜ë³µì ìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.

    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸ ë¬¸ìì—´.

    Returns:
        ì§€ì •ëœ ë’¤ìª½ êµ¬ë‘ì ì´ ì œê±°ëœ í…ìŠ¤íŠ¸ ë¬¸ìì—´.
    """
    text = text.rstrip()
    for char in sentence_end_marks:
        # ì—¬ëŸ¬ ê°œì¼ ê²½ìš°(ì˜ˆ: "!!")ë¥¼ ëŒ€ë¹„í•´ ê° êµ¬ë‘ì ì„ ë°˜ë³µì ìœ¼ë¡œ ì œê±°
        while text.endswith(char):
             text = text.rstrip(char)
    return text # ì œê±°ëœ í…ìŠ¤íŠ¸ ë°˜í™˜

def find_matching_texts(texts_without_punctuation: collections.deque) -> list[tuple[str, str]]:
    """
    êµ¬ë‘ì ì´ ì œê±°ëœ í…ìŠ¤íŠ¸ê°€ ë™ì¼í•œ ìµœê·¼ ì—°ì† í•­ëª©ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.

    (original_text, stripped_text) íŠœí”Œì˜ ë°í¬ë¥¼ ë’¤ì—ì„œë¶€í„° ë°˜ë³µí•©ë‹ˆë‹¤.
    *ë§ˆì§€ë§‰* í•­ëª©ì˜ ì œê±°ëœ í…ìŠ¤íŠ¸ì™€ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  í•­ëª©ì„ ìˆ˜ì§‘í•˜ê³ ,
    ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ì œê±°ëœ í…ìŠ¤íŠ¸ë¥¼ ë§Œë‚˜ë©´ ì¤‘ë‹¨í•©ë‹ˆë‹¤.

    Args:
        texts_without_punctuation: ê° íŠœí”Œì´ (original_text, stripped_text)ì¸
                                   íŠœí”Œì˜ ë°í¬.

    Returns:
        ì¼ì¹˜í•˜ëŠ” (original_text, stripped_text) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸ (ê°€ì¥ ì˜¤ë˜ëœ ì¼ì¹˜ í•­ëª©ì´ ë¨¼ì €).
        ì…ë ¥ ë°í¬ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not texts_without_punctuation:
        return []

    # ë§ˆì§€ë§‰ í•­ëª©ì—ì„œ ì œê±°ëœ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    last_stripped_text = texts_without_punctuation[-1][1]

    matching_entries = []

    # ë°í¬ë¥¼ ë’¤ì—ì„œë¶€í„° ë°˜ë³µ
    for entry in reversed(texts_without_punctuation):
        original_text, stripped_text = entry

        # ë¶ˆì¼ì¹˜ë¥¼ ì°¾ìœ¼ë©´ ìˆ˜ì§‘ ì¤‘ë‹¨
        if stripped_text != last_stripped_text:
            break

        # ì¼ì¹˜í•˜ëŠ” í•­ëª© ì¶”ê°€ (ë‚˜ì¤‘ì— ë’¤ì§‘ì„ ê²ƒ)
        matching_entries.append(entry)

    # ì›ë˜ ì‚½ì… ìˆœì„œë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ê²°ê³¼ ë’¤ì§‘ê¸°
    matching_entries.reverse()

    return matching_entries

def interpolate_detection(prob: float) -> float:
    """
    ë¯¸ë¦¬ ì •ì˜ëœ ì•µì»¤ í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í™•ë¥ ì— ê¸°ë°˜í•œ ê°’ì„ ì„ í˜• ë³´ê°„í•©ë‹ˆë‹¤.

    ì…ë ¥ í™•ë¥  `prob`(0.0ê³¼ 1.0 ì‚¬ì´ë¡œ ì œí•œë¨)ì„ `anchor_points` ë¦¬ìŠ¤íŠ¸ì—
    ê¸°ë°˜í•œ ì¶œë ¥ ê°’ìœ¼ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤. `prob`ì´ ì†í•˜ëŠ” `anchor_points` ë‚´ì˜
    ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì°¾ì•„ ì„ í˜• ë³´ê°„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        prob: ì…ë ¥ í™•ë¥ , 0.0ê³¼ 1.0 ì‚¬ì´ë¡œ ì˜ˆìƒë¨.

    Returns:
        ë³´ê°„ëœ ê°’. ë³´ê°„ì— ì‹¤íŒ¨í•˜ë©´ ëŒ€ì²´ ê°’ìœ¼ë¡œ 4.0ì„ ë°˜í™˜í•©ë‹ˆë‹¤
        (ì•µì»¤ í¬ì¸íŠ¸ê°€ [0,1] ë²”ìœ„ë¥¼ í¬í•¨í•˜ë©´ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨).
    """
    # ë§Œì•½ì„ ìœ„í•´ í™•ë¥ ì„ 0.0ê³¼ 1.0 ì‚¬ì´ë¡œ ì œí•œ
    p = max(0.0, min(prob, 1.0))

    # í™•ë¥ ì´ ì•µì»¤ í¬ì¸íŠ¸ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
    for ap_p, ap_val in anchor_points:
        if abs(ap_p - p) < 1e-9: # ë¶€ë™ ì†Œìˆ˜ì  ë¹„êµë¥¼ ìœ„í•´ í—ˆìš© ì˜¤ì°¨ ì‚¬ìš©
            return ap_val

    # pê°€ ì†í•˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ [p1, p2] ì°¾ê¸°
    for i in range(len(anchor_points) - 1):
        p1, v1 = anchor_points[i]
        p2, v2 = anchor_points[i+1]
        # ì´ ë¡œì§ì´ ì‘ë™í•˜ë ¤ë©´ ì•µì»¤ í¬ì¸íŠ¸ê°€ í™•ë¥  ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì–´ì•¼ í•¨
        if p1 <= p <= p2:
            # ì•µì»¤ í¬ì¸íŠ¸ê°€ ë™ì¼í•œ í™•ë¥ ì„ ê°€ì§ˆ ê²½ìš° 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€
            if abs(p2 - p1) < 1e-9:
                return v1 # ë˜ëŠ” v2, p1=p2ì´ë©´ ë¹„ìŠ·í•´ì•¼ í•¨
            # ì„ í˜• ë³´ê°„ ê³µì‹
            ratio = (p - p1) / (p2 - p1)
            return v1 + ratio * (v2 - v1)

    # ëŒ€ì²´: anchor_pointsê°€ [0,1]ì„ ì œëŒ€ë¡œ í¬í•¨í•˜ë©´ ë„ë‹¬í•´ì„œëŠ” ì•ˆ ë¨.
    logger.warning(f"ğŸ¤âš ï¸ í™•ë¥  {p}ê°€ ì •ì˜ëœ ì•µì»¤ í¬ì¸íŠ¸ {anchor_points}ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. ëŒ€ì²´ ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
    return 4.0

class TurnDetection:
    """
    í…ìŠ¤íŠ¸ ì…ë ¥ê³¼ ë¬¸ì¥ ì™„ì„± ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë°œí™”ì ì „í™˜ ê°ì§€ ë¡œì§ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

    ì´ í´ë˜ìŠ¤ëŠ” í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë°›ì•„ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ì™„ì„±
    í™•ë¥ ì„ ì˜ˆì¸¡í•˜ê³ , êµ¬ë‘ì ì„ ê³ ë ¤í•˜ì—¬ ë‹¤ìŒ ë°œí™”ìê°€ ì‹œì‘í•˜ê¸° ì „ì— ì œì•ˆëœ
    ëŒ€ê¸° ì‹œê°„(ì •ì§€ ê¸°ê°„)ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì²˜ë¦¬ë¥¼ ìœ„í•´ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ê³ 
    ìƒˆë¡œìš´ ëŒ€ê¸° ì‹œê°„ ì œì•ˆì— ëŒ€í•œ ì½œë°±ì„ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ ìµœê·¼ í…ìŠ¤íŠ¸ì˜ ì´ë ¥ì„
    ìœ ì§€í•˜ê³  ëª¨ë¸ ì˜ˆì¸¡ì„ ìœ„í•´ ìºì‹±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        on_new_waiting_time: callable,
        local: bool = False,
        pipeline_latency: float = 0.5,
        pipeline_latency_overhead: float = 0.1,
    ) -> None:
        """
        TurnDetection ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³ , ë‚´ë¶€ ìƒíƒœ(ë°í¬, ìºì‹œ)ë¥¼ ì„¤ì •í•˜ê³ ,
        ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ìŠ¤ë ˆë“œë¥¼ ì‹œì‘í•˜ê³ , ëª¨ë¸ ì›Œë°ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            on_new_waiting_time: ìƒˆë¡œìš´ ëŒ€ê¸° ì‹œê°„ì´ ê³„ì‚°ë  ë•Œ í˜¸ì¶œë˜ëŠ” ì½œë°± í•¨ìˆ˜.
                                 `(time: float, text: str)`ì„ ë°›ìŠµë‹ˆë‹¤.
            local: Trueì´ë©´ `model_dir_local`ì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ `model_dir_cloud`ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.
            pipeline_latency: STT/ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ ì˜ˆìƒ ê¸°ë³¸ ì§€ì—° ì‹œê°„(ì´ˆ).
            pipeline_latency_overhead: íŒŒì´í”„ë¼ì¸ ì§€ì—° ì‹œê°„ì— ì¶”ê°€ë˜ëŠ” ë²„í¼.
        """
        model_dir = model_dir_local if local else model_dir_cloud

        self.on_new_waiting_time = on_new_waiting_time

        self.current_waiting_time: float = -1 # ë§ˆì§€ë§‰ìœ¼ë¡œ ì œì•ˆëœ ì‹œê°„ì„ ì¶”ì 
        # íš¨ìœ¨ì ì´ê³  ì œí•œëœ ì´ë ¥ ì €ì¥ì„ ìœ„í•´ maxlenì„ ê°€ì§„ ë°í¬ ì‚¬ìš©
        self.text_time_deque: collections.deque[tuple[float, str]] = collections.deque(maxlen=100)
        self.texts_without_punctuation: collections.deque[tuple[str, str]] = collections.deque(maxlen=20)

        self.text_queue: queue.Queue[str] = queue.Queue() # ë“¤ì–´ì˜¤ëŠ” í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ í
        self.text_worker = threading.Thread(
            target=self._text_worker,
            daemon=True # ì´ ìŠ¤ë ˆë“œê°€ ì‹¤í–‰ ì¤‘ì´ë”ë¼ë„ í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë  ìˆ˜ ìˆë„ë¡ í—ˆìš©
        )
        self.text_worker.start()

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ğŸ¤ğŸ”Œ ì‚¬ìš© ì¥ì¹˜: {self.device}")
        self.tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(model_dir)
        self.classification_model = transformers.DistilBertForSequenceClassification.from_pretrained(model_dir)
        self.classification_model.to(self.device)
        self.classification_model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.max_length: int = 128 # ëª¨ë¸ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        self.pipeline_latency: float = pipeline_latency
        self.pipeline_latency_overhead: float = pipeline_latency_overhead

        # LRU ë™ì‘ì„ ìœ„í•´ OrderedDictë¡œ ì™„ë£Œ í™•ë¥  ìºì‹œ ì´ˆê¸°í™”
        self._completion_probability_cache: collections.OrderedDict[str, float] = collections.OrderedDict()
        self._completion_probability_cache_max_size: int = 256 # LRU ìºì‹œì˜ ìµœëŒ€ í¬ê¸°

        # ë¹ ë¥¸ ì´ˆê¸° ì˜ˆì¸¡ì„ ìœ„í•´ ë¶„ë¥˜ ëª¨ë¸ ì›Œë°ì—…
        logger.info("ğŸ¤ğŸ”¥ ë¶„ë¥˜ ëª¨ë¸ ì›Œë°ì—… ì¤‘...")
        with torch.no_grad():
            warmup_text = "This is a warmup sentence."
            inputs = self.tokenizer(
                warmup_text,
                return_tensors="pt",
                truncation=True,
                padding="max_length",
                max_length=self.max_length
            )
            inputs = {key: value.to(self.device) for key, value in inputs.items()}
            _ = self.classification_model(**inputs) # ì˜ˆì¸¡ í•œ ë²ˆ ì‹¤í–‰
        logger.info("ğŸ¤âœ… ë¶„ë¥˜ ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ.")

        # ê¸°ë³¸ ë™ì  ì •ì§€ ì„¤ì • (speed_factor=0.0ì— ëŒ€í•´ ì´ˆê¸°í™”ë¨)
        self.detection_speed: float = 0.5
        self.ellipsis_pause: float = 2.3
        self.punctuation_pause: float = 0.39
        self.exclamation_pause: float = 0.35
        self.question_pause: float = 0.33
        self.unknown_sentence_detection_pause: float = 1.25
        # ì´ˆê¸° ì„¤ì • ì ìš© (ë‚˜ì¤‘ì— ë‹¤ì‹œ í˜¸ì¶œ ê°€ëŠ¥)
        self.update_settings(speed_factor=0.0)

    def update_settings(self, speed_factor: float) -> None:
        """
        ì†ë„ ê³„ìˆ˜ì— ë”°ë¼ ë™ì  ì •ì§€ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.

        ê³„ì‚°ì— ì‚¬ìš©ë˜ëŠ” ë‹¤ì–‘í•œ ì •ì§€ ê¸°ê°„ì— ëŒ€í•´ 'ë¹ ë¦„'(speed_factor=0.0)ê³¼
        'ë§¤ìš° ëŠë¦¼'(speed_factor=1.0) ì„¤ì • ì‚¬ì´ë¥¼ ì„ í˜• ë³´ê°„í•©ë‹ˆë‹¤.
        speed_factorë¥¼ 0.0ê³¼ 1.0 ì‚¬ì´ë¡œ ì œí•œí•©ë‹ˆë‹¤.

        Args:
            speed_factor: ë¯¸ë¦¬ ì •ì˜ëœ ì„¤ì • ì‚¬ì´ì˜ ë³´ê°„ì„ ì œì–´í•˜ëŠ” 0.0(ê°€ì¥ ë¹ ë¦„)ê³¼
                          1.0(ê°€ì¥ ëŠë¦¼) ì‚¬ì´ì˜ ë¶€ë™ ì†Œìˆ˜ì .
        """
        speed_factor = max(0.0, min(speed_factor, 1.0)) # ê³„ìˆ˜ ì œí•œ

        # ê¸°ë³¸ 'ë¹ ë¦„' ì„¤ì • (speed_factor = 0.0)
        fast = {
            'detection_speed': 0.5,
            'ellipsis_pause': 2.3,
            'punctuation_pause': 0.39,
            'exclamation_pause': 0.35,
            'question_pause': 0.33,
            'unknown_sentence_detection_pause': 1.25
        }

        # ëª©í‘œ 'ë§¤ìš° ëŠë¦¼' ì„¤ì • (speed_factor = 1.0)
        very_slow = {
            'detection_speed': 1.7,
            'ellipsis_pause': 3.0,
            'punctuation_pause': 0.9,
            'exclamation_pause': 0.8,
            'question_pause': 0.8,
            'unknown_sentence_detection_pause': 1.9
        }

        # ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ì„ í˜• ë³´ê°„
        self.detection_speed = fast['detection_speed'] + speed_factor * (very_slow['detection_speed'] - fast['detection_speed'])
        self.ellipsis_pause = fast['ellipsis_pause'] + speed_factor * (very_slow['ellipsis_pause'] - fast['ellipsis_pause'])
        self.punctuation_pause = fast['punctuation_pause'] + speed_factor * (very_slow['punctuation_pause'] - fast['punctuation_pause'])
        self.exclamation_pause = fast['exclamation_pause'] + speed_factor * (very_slow['exclamation_pause'] - fast['exclamation_pause'])
        self.question_pause = fast['question_pause'] + speed_factor * (very_slow['question_pause'] - fast['question_pause'])
        self.unknown_sentence_detection_pause = fast['unknown_sentence_detection_pause'] + speed_factor * (very_slow['unknown_sentence_detection_pause'] - fast['unknown_sentence_detection_pause'])
        logger.info(f"ğŸ¤âš™ï¸ speed_factor={speed_factor:.2f}ë¡œ ë°œí™”ì ì „í™˜ ê°ì§€ ì„¤ì • ì—…ë°ì´íŠ¸ë¨")


    def suggest_time(
            self,
            time_val: float, # ë‚´ì¥ í•¨ìˆ˜ì™€ ê²¹ì¹˜ì§€ ì•Šë„ë¡ 'time'ì—ì„œ ì´ë¦„ ë³€ê²½
            text: str = None
        ) -> None:
        """
        ì œì•ˆëœ ì •ì§€ ê¸°ê°„ìœ¼ë¡œ `on_new_waiting_time` ì½œë°±ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.

        ì¤‘ë³µ í˜¸ì¶œì„ í”¼í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì œì•ˆ ì‹œê°„ì´ í˜„ì¬ ì €ì¥ëœ
        `current_waiting_time`ê³¼ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ ì½œë°±ì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.

        Args:
            time_val: ê³„ì‚°ëœ ì œì•ˆ ëŒ€ê¸° ì‹œê°„(ì´ˆ).
            text: ì´ ëŒ€ê¸° ì‹œê°„ ê³„ì‚°ê³¼ ê´€ë ¨ëœ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸.
        """
        if time_val == self.current_waiting_time:
            return # ë³€ê²½ ì—†ìŒ, ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ

        self.current_waiting_time = time_val

        if self.on_new_waiting_time:
            self.on_new_waiting_time(time_val, text)

    def get_completion_probability(
        self,
        sentence: str
    ) -> float:
        """
        ML ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ë¬¸ì¥ì´ ì™„ì „í•  í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ë‚´ë¶€ LRU ìºì‹œ(`_completion_probability_cache`)ë¥¼ ì‚¬ìš©í•˜ì—¬
        ì´ì „ì— ë³¸ ë¬¸ì¥ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.

        Args:
            sentence: ë¶„ì„í•  ì…ë ¥ ë¬¸ì¥ ë¬¸ìì—´.

        Returns:
            ëª¨ë¸ì— ì˜í•´ ë¬¸ì¥ì´ ì™„ì „í•˜ë‹¤ê³  ê°„ì£¼ë  í™•ë¥ (0.0ê³¼ 1.0 ì‚¬ì´)ì„
            ë‚˜íƒ€ë‚´ëŠ” ë¶€ë™ ì†Œìˆ˜ì .
        """
        # ë¨¼ì € ìºì‹œ í™•ì¸
        if sentence in self._completion_probability_cache:
            self._completion_probability_cache.move_to_end(sentence) # ìµœê·¼ ì‚¬ìš©ìœ¼ë¡œ í‘œì‹œ
            return self._completion_probability_cache[sentence]

        # ìºì‹œì— ì—†ìœ¼ë©´ ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰
        import torch
        import torch.nn.functional as F

        inputs = self.tokenizer(
            sentence,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=self.max_length
        )
        # ì…ë ¥ í…ì„œë¥¼ ì˜¬ë°”ë¥¸ ì¥ì¹˜(CPU ë˜ëŠ” GPU)ë¡œ ì´ë™
        inputs = {key: value.to(self.device) for key, value in inputs.items()}

        with torch.no_grad(): # ì¶”ë¡ ì„ ìœ„í•´ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
            outputs = self.classification_model(**inputs)

        logits = outputs.logits
        # softmaxë¥¼ ì ìš©í•˜ì—¬ í™•ë¥  [prob_incomplete, prob_complete] ì–»ê¸°
        probabilities = F.softmax(logits, dim=1).squeeze().tolist()
        prob_complete = probabilities[1] # ì¸ë±ìŠ¤ 1ì€ 'complete' ë ˆì´ë¸”ì— í•´ë‹¹

        # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        self._completion_probability_cache[sentence] = prob_complete
        self._completion_probability_cache.move_to_end(sentence) # ìµœê·¼ ì‚¬ìš©ìœ¼ë¡œ í‘œì‹œ

        # ìºì‹œ í¬ê¸° ìœ ì§€ (LRU ì¶•ì¶œ)
        if len(self._completion_probability_cache) > self._completion_probability_cache_max_size:
            self._completion_probability_cache.popitem(last=False) # ê°€ì¥ ìµœê·¼ì— ì‚¬ìš©ë˜ì§€ ì•Šì€ í•­ëª© ì œê±°

        return prob_complete

    def get_suggested_whisper_pause(self, text: str) -> float:
        """
        í…ìŠ¤íŠ¸ì˜ ë êµ¬ë‘ì ì— ë”°ë¼ ê¸°ë³¸ ì •ì§€ ê¸°ê°„ì„ ê²°ì •í•©ë‹ˆë‹¤.

        íŠ¹ì • ë íŒ¨í„´('...', '.', '!', '?')ì„ í™•ì¸í•˜ê³  ì¸ìŠ¤í„´ìŠ¤ì˜ ì„¤ì •ì—
        ì •ì˜ëœ í•´ë‹¹ ì •ì§€ ê¸°ê°„(ì˜ˆ: `self.ellipsis_pause`)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        íŠ¹ì • êµ¬ë‘ì ì´ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ `self.unknown_sentence_detection_pause`ë¥¼
        ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸ ë¬¸ìì—´.

        Returns:
            ë êµ¬ë‘ì ì— ë”°ë¥¸ ì œì•ˆëœ ì •ì§€ ê¸°ê°„(ì´ˆ).
        """
        if ends_with_string(text, "..."):
            return self.ellipsis_pause
        elif ends_with_string(text, "."):
            return self.punctuation_pause
        elif ends_with_string(text, "!"):
            return self.exclamation_pause
        elif ends_with_string(text, "?"):
            return self.question_pause
        else:
            # íŠ¹ì • ëì´ ê°ì§€ë˜ì§€ ì•ŠìŒ, ì•Œ ìˆ˜ ì—†ëŠ” ëì— ëŒ€í•œ ì¼ë°˜ ì •ì§€ ì‚¬ìš©
            return self.unknown_sentence_detection_pause

    def _text_worker(
        self
    ) -> None:
        """
        íì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ ë°œí™”ì ì „í™˜ì„ ê°ì§€í•˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ìŠ¤ë ˆë“œ.

        `self.text_queue`ì—ì„œ í…ìŠ¤íŠ¸ í•­ëª©ì„ ê³„ì† ê²€ìƒ‰í•©ë‹ˆë‹¤. ê° í•­ëª©ì— ëŒ€í•´:
        1. í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
        2. í…ìŠ¤íŠ¸ ì´ë ¥ ë°í¬ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        3. êµ¬ë‘ì  ì¼ê´€ì„±ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ìµœê·¼ ì¼ì¹˜í•˜ëŠ” í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        4. ì¼ì¹˜ í•­ëª©ì—ì„œ ê´€ì°°ëœ êµ¬ë‘ì ì„ ê¸°ë°˜ìœ¼ë¡œ í‰ê·  ì •ì§€ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        5. ë¬¸ì¥ ì™„ì„± ëª¨ë¸ì„ ìœ„í•´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
        6. ëª¨ë¸ì—ì„œ ì™„ì„± í™•ë¥ ì„ ì–»ìŠµë‹ˆë‹¤(ìºì‹œ ì‚¬ìš©).
        7. ëª¨ë¸ì˜ í™•ë¥ ì„ ë‹¤ë¥¸ ì •ì§€ ê°’ìœ¼ë¡œ ë³´ê°„í•©ë‹ˆë‹¤.
        8. ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ë‘ì  ê¸°ë°˜ ì •ì§€ì™€ ëª¨ë¸ ê¸°ë°˜ ì •ì§€ë¥¼ ê²°í•©í•©ë‹ˆë‹¤.
        9. ì†ë„ ê³„ìˆ˜ ë° ì¡°ì •(ì˜ˆ: ë§ì¤„ì„í‘œ)ì„ ì ìš©í•©ë‹ˆë‹¤.
        10. ìµœì¢… ì •ì§€ê°€ ìµœì†Œ íŒŒì´í”„ë¼ì¸ ì§€ì—° ì‹œê°„ ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        11. ìµœì¢… ê³„ì‚°ëœ ì •ì§€ ê¸°ê°„ìœ¼ë¡œ `suggest_time`ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
        ì ì¬ì ì¸ ì¢…ë£Œë¥¼ í—ˆìš©í•˜ê¸° ìœ„í•´ í íƒ€ì„ì•„ì›ƒì„ ë¶€ë“œëŸ½ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        while True:
            try:
                # íì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë‹¤ë¦¬ë˜, ì˜ì›íˆ ì°¨ë‹¨ë˜ì§€ ì•Šë„ë¡ íƒ€ì„ì•„ì›ƒ ì„¤ì •
                text = self.text_queue.get(block=True, timeout=0.1)
            except queue.Empty:
                # íƒ€ì„ì•„ì›ƒ ë‚´ì— í…ìŠ¤íŠ¸ë¥¼ ë°›ì§€ ëª»í•¨, ë‹¤ì‹œ ë°˜ë³µ
                time.sleep(0.01) # ìœ íœ´ ìƒíƒœì¼ ë•Œ CPUë¥¼ ì–‘ë³´í•˜ê¸° ìœ„í•œ ì‘ì€ sleep
                continue

            # --- í…ìŠ¤íŠ¸ë¥¼ ë°›ìœ¼ë©´ ì²˜ë¦¬ ì‹œì‘ ---
            logger.info(f"ğŸ¤âš™ï¸ \"{text}\"ì— ëŒ€í•œ ì •ì§€ ê³„ì‚° ì‹œì‘")
            
            processed_text = preprocess_text(text) # ì´ˆê¸° ì •ë¦¬ ì ìš©

            # ì´ë ¥ ë°í¬ ì—…ë°ì´íŠ¸
            current_time = time.time()
            self.text_time_deque.append((current_time, processed_text))
            text_without_punctuation = strip_ending_punctuation(processed_text)
            self.texts_without_punctuation.append((processed_text, text_without_punctuation))

            # ì¼ê´€ëœ êµ¬ë‘ì  ì •ì§€ë¥¼ ìœ„í•´ ìµœê·¼ ì¼ì¹˜í•˜ëŠ” í…ìŠ¤íŠ¸ ë¶„ì„
            matches = find_matching_texts(self.texts_without_punctuation)

            added_pauses = 0
            contains_ellipses = False
            if matches: # matchesê°€ ë¹„ì–´ ìˆì„ ë•Œ 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€
                for i, match in enumerate(matches):
                    same_text, _ = match # ì—¬ê¸°ì„œëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ë§Œ í•„ìš”
                    whisper_suggested_pause_match = self.get_suggested_whisper_pause(same_text)
                    added_pauses += whisper_suggested_pause_match
                    if ends_with_string(same_text, "..."):
                        contains_ellipses = True
                # ìµœê·¼ ì¼ê´€ëœ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‰ê·  ì •ì§€ ê³„ì‚°
                avg_pause = added_pauses / len(matches)
            else:
                # ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ í˜„ì¬ í…ìŠ¤íŠ¸ê°€ ì œì•ˆí•˜ëŠ” ì •ì§€ë¥¼ ì§ì ‘ ì‚¬ìš©
                 avg_pause = self.get_suggested_whisper_pause(processed_text)
                 if ends_with_string(processed_text, "..."):
                    contains_ellipses = True

            whisper_suggested_pause = avg_pause # í‰ê· ëœ ì •ì§€ ì‚¬ìš©

            # ë¬¸ì¥ ì™„ì„± ëª¨ë¸ì„ ìœ„í•´ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ëª¨ë“  êµ¬ë‘ì  ì œê±°)
            import string
            transtext = processed_text.translate(str.maketrans('', '', string.punctuation))
            # ëì— ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” ì•ŒíŒŒë²³/ìˆ«ìê°€ ì•„ë‹Œ ë¬¸ì ì¶”ê°€ ì •ë¦¬
            cleaned_for_model = re.sub(r'[^a-zA-Z\s]+$', '', transtext).rstrip() # ë’¤ë”°ë¥´ëŠ” ê³µë°±ë„ ì œê±°

            # ë¬¸ì¥ ì™„ì„± í™•ë¥  ì–»ê¸°
            prob_complete = self.get_completion_probability(cleaned_for_model)

            # í™•ë¥ ì„ ì •ì§€ ê¸°ê°„ìœ¼ë¡œ ë³´ê°„
            sentence_finished_model_pause = interpolate_detection(prob_complete)

            # ì •ì§€ ê²°í•©: êµ¬ë‘ì  ì •ì§€ì— ë” ë§ì€ ì¤‘ìš”ë„ë¥¼ ë‘ëŠ” ê°€ì¤‘ í‰ê· 
            weight_towards_whisper = 0.65
            weighted_pause = (weight_towards_whisper * whisper_suggested_pause +
                             (1 - weight_towards_whisper) * sentence_finished_model_pause)

            # ì „ì²´ ì†ë„ ê³„ìˆ˜ ì ìš©
            final_pause = weighted_pause * self.detection_speed

            # ìµœê·¼ì— ë§ì¤„ì„í‘œê°€ ê°ì§€ëœ ê²½ìš° ì•½ê°„ì˜ ì¶”ê°€ ì •ì§€ ì¶”ê°€
            if contains_ellipses:
                final_pause += 0.2

            logger.info(f"ğŸ¤ğŸ“Š ê³„ì‚°ëœ ì •ì§€: êµ¬ë‘ì ={whisper_suggested_pause:.2f}, ëª¨ë¸={sentence_finished_model_pause:.2f}, ê°€ì¤‘ì¹˜={weighted_pause:.2f}, ìµœì¢…={final_pause:.2f} (\"{processed_text}\", í™•ë¥ ={prob_complete:.2f})")


            # ìµœì¢… ì •ì§€ê°€ íŒŒì´í”„ë¼ì¸ ì§€ì—° ì‹œê°„ ì˜¤ë²„í—¤ë“œë³´ë‹¤ ì‘ì§€ ì•Šì€ì§€ í™•ì¸
            min_pause = self.pipeline_latency + self.pipeline_latency_overhead
            if final_pause < min_pause:
                logger.info(f"ğŸ¤âš ï¸ ìµœì¢… ì •ì§€({final_pause:.2f}s)ê°€ ìµœì†Œê°’({min_pause:.2f}s)ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤. ìµœì†Œê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                final_pause = min_pause
            
            # ì½œë°±ì„ í†µí•´ ê³„ì‚°ëœ ì‹œê°„ ì œì•ˆ
            self.suggest_time(final_pause, processed_text) # ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ„í•´ processed_text ì‚¬ìš©

            # íì— ëŒ€í•´ ì‘ì—… ì™„ë£Œ í‘œì‹œ (queue.join() ì‚¬ìš© ì‹œ ì¤‘ìš”)
            self.text_queue.task_done()

    def calculate_waiting_time(
            self,
            text: str) -> None:
        """
        ëŒ€ê¸° ì‹œê°„ ê³„ì‚°ì„ ìœ„í•´ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì²˜ë¦¬ íì— ì¶”ê°€í•©ë‹ˆë‹¤.

        ì´ê²ƒì€ í…ìŠ¤íŠ¸ë¥¼ ë°œí™”ì ì „í™˜ ê°ì§€ ì‹œìŠ¤í…œì— ê³µê¸‰í•˜ëŠ” ì§„ì…ì ì…ë‹ˆë‹¤.
        ì‹¤ì œ ê³„ì‚°ì€ ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ìŠ¤ë ˆë“œì—ì„œ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë°œìƒí•©ë‹ˆë‹¤.

        Args:
            text: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ (ì˜ˆ: STTì—ì„œ).
        """
        logger.info(f"ğŸ¤ğŸ“¥ ì •ì§€ ê³„ì‚°ì„ ìœ„í•´ í…ìŠ¤íŠ¸ íì— ì¶”ê°€: \"{text}\"")
        self.text_queue.put(text)

    def reset(self) -> None:
        """
        TurnDetection ì¸ìŠ¤í„´ìŠ¤ì˜ ë‚´ë¶€ ìƒíƒœë¥¼ ë¦¬ì…‹í•©ë‹ˆë‹¤.

        í…ìŠ¤íŠ¸ ì´ë ¥ ë°í¬, ëª¨ë¸ ì˜ˆì¸¡ ìºì‹œë¥¼ ì§€ìš°ê³  í˜„ì¬ ëŒ€ê¸° ì‹œê°„ ì¶”ì ê¸°ë¥¼
        ë¦¬ì…‹í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ëŒ€í™”ë‚˜ ìƒí˜¸ì‘ìš© ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‹œì‘í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.
        """
        logger.info("ğŸ¤ğŸ”„ TurnDetection ìƒíƒœ ë¦¬ì…‹ ì¤‘.")
        # ì´ë ¥ ë°í¬ ì§€ìš°ê¸°
        self.text_time_deque.clear()
        self.texts_without_punctuation.clear()
        # ë§ˆì§€ë§‰ìœ¼ë¡œ ì œì•ˆëœ ì‹œê°„ ë¦¬ì…‹
        self.current_waiting_time = -1
        # ì˜ˆì¸¡ ìºì‹œ ì§€ìš°ê¸°
        if hasattr(self, "_completion_probability_cache"):
            self._completion_probability_cache.clear()
        # ì²˜ë¦¬ í ì§€ìš°ê¸° (ì„ íƒ ì‚¬í•­, ì²˜ë¦¬ë˜ì§€ ì•Šì€ í•­ëª©ì„ ë²„ë¦´ ìˆ˜ ìˆìŒ)
        # while not self.text_queue.empty():
        #     try:
        #         self.text_queue.get_nowait()
        #         self.text_queue.task_done()
        #     except queue.Empty:
        #         break
